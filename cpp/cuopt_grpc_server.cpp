/*
 * SPDX-FileCopyrightText: Copyright (c) 2025-2026, NVIDIA CORPORATION & AFFILIATES. All rights
 * reserved. SPDX-License-Identifier: Apache-2.0
 */

/**
 * @file cuopt_grpc_server.cpp
 * @brief gRPC-based remote solve server with full worker process infrastructure
 *
 * This server uses gRPC for client communication but preserves the
 * existing worker process infrastructure:
 * - Worker processes with shared memory job queues
 * - Pipe-based IPC for problem/result data
 * - Result tracking and retrieval threads
 * - Log streaming
 *
 * Only the client-facing network layer is different (gRPC vs TCP).
 */

#ifdef CUOPT_ENABLE_GRPC

#include <grpcpp/grpcpp.h>
#include "cuopt_remote.pb.h"
#include "cuopt_remote_service.grpc.pb.h"

#include <cuopt/linear_programming/solve.hpp>
#include <cuopt/linear_programming/utilities/internals.hpp>
#include <cuopt/linear_programming/utilities/remote_serialization.hpp>
#include <mps_parser/mps_data_model.hpp>

#include <cuda_runtime.h>
#include <fcntl.h>
#include <poll.h>
#include <sys/mman.h>
#include <sys/stat.h>
#include <sys/wait.h>
#include <unistd.h>
#include <cstdio>

#include <algorithm>
#include <atomic>
#include <cerrno>
#include <chrono>
#include <condition_variable>
#include <csignal>
#include <cstdlib>
#include <cstring>
#include <fstream>
#include <iostream>
#include <limits>
#include <map>
#include <memory>
#include <mutex>
#include <random>
#include <sstream>
#include <string>
#include <thread>
#include <vector>

using grpc::Server;
using grpc::ServerBuilder;
using grpc::ServerContext;
using grpc::ServerReaderWriter;
using grpc::ServerWriter;
using grpc::Status;
using grpc::StatusCode;

using namespace cuopt::linear_programming;
// Note: NOT using "using namespace cuopt::remote" to avoid JobStatus enum conflict

// ============================================================================
// Shared Memory Structures (must match between main process and workers)
// ============================================================================

constexpr size_t MAX_JOBS    = 100;
constexpr size_t MAX_RESULTS = 100;

template <size_t N>
void copy_cstr(char (&dst)[N], const std::string& src)
{
  std::snprintf(dst, N, "%s", src.c_str());
}

template <size_t N>
void copy_cstr(char (&dst)[N], const char* src)
{
  std::snprintf(dst, N, "%s", src ? src : "");
}

// Job queue entry - small fixed size, data stored in separate per-job shared memory or sent via
// pipe
struct JobQueueEntry {
  char job_id[64];
  uint32_t problem_type;          // 0 = LP, 1 = MIP
  uint64_t data_size;             // Size of problem data (uint64 for large problems)
  char shm_data_name[128];        // Name of per-job shared memory segment (shm mode only)
  std::atomic<bool> ready;        // Job is ready to be processed
  std::atomic<bool> claimed;      // Worker has claimed this job
  std::atomic<pid_t> worker_pid;  // PID of worker that claimed this job (0 if none)
  std::atomic<bool> cancelled;    // Job has been cancelled (worker should skip)
  // Pipe mode fields
  std::atomic<int> worker_index;  // Index of worker that claimed this job (-1 if none)
  std::atomic<bool> data_sent;    // Server has sent data to worker's pipe (pipe mode)
};

// Result queue entry - small fixed size, data stored in separate per-result shared memory or pipe
struct ResultQueueEntry {
  char job_id[64];
  uint32_t status;          // 0 = success, 1 = error, 2 = cancelled
  uint64_t data_size;       // Size of result data (uint64 for large results)
  char shm_data_name[128];  // Name of per-result shared memory segment (shm mode only)
  char error_message[1024];
  std::atomic<bool> ready;        // Result is ready
  std::atomic<bool> retrieved;    // Result has been retrieved
  std::atomic<int> worker_index;  // Index of worker that produced this result (pipe mode)
};

// Shared memory control block
struct SharedMemoryControl {
  std::atomic<bool> shutdown_requested;
  std::atomic<int> active_workers;
};

// ============================================================================
// Job status tracking (main process only)
// ============================================================================

enum class JobStatus { QUEUED, PROCESSING, COMPLETED, FAILED, NOT_FOUND, CANCELLED };

struct IncumbentEntry {
  double objective = 0.0;
  std::vector<double> assignment;
};

struct JobInfo {
  std::string job_id;
  JobStatus status;
  std::chrono::steady_clock::time_point submit_time;
  std::vector<uint8_t> result_data;
  std::vector<IncumbentEntry> incumbents;
  bool is_mip;
  std::string error_message;
  bool is_blocking;
};

struct JobWaiter {
  std::mutex mutex;
  std::condition_variable cv;
  std::vector<uint8_t> result_data;
  std::string error_message;
  bool success;
  bool ready;
  std::atomic<int> waiters{0};
  JobWaiter() : success(false), ready(false) {}
};

// ============================================================================
// Global state
// ============================================================================

std::atomic<bool> keep_running{true};
std::map<std::string, JobInfo> job_tracker;
std::mutex tracker_mutex;
std::condition_variable result_cv;

std::map<std::string, std::shared_ptr<JobWaiter>> waiting_threads;
std::mutex waiters_mutex;

JobQueueEntry* job_queue       = nullptr;
ResultQueueEntry* result_queue = nullptr;
SharedMemoryControl* shm_ctrl  = nullptr;

std::vector<pid_t> worker_pids;

struct ServerConfig {
  int port            = 8765;
  int num_workers     = 1;
  bool verbose        = true;
  bool use_pipes      = true;
  bool log_to_console = false;
  // gRPC max message size in MiB. 0 => unlimited (gRPC uses -1 internally).
  int max_message_mb  = 256;
  bool enable_tls     = false;
  bool require_client = false;
  std::string tls_cert_path;
  std::string tls_key_path;
  std::string tls_root_path;
};

ServerConfig config;

struct WorkerPipes {
  int to_worker_fd;
  int from_worker_fd;
  int worker_read_fd;
  int worker_write_fd;
  int incumbent_from_worker_fd;
  int worker_incumbent_write_fd;
};

std::vector<WorkerPipes> worker_pipes;

std::mutex pending_data_mutex;
std::map<std::string, std::vector<uint8_t>> pending_job_data;

// Large payloads uploaded via gRPC streaming are spooled to disk to avoid
// holding multi-GB request buffers in the server process.
struct PendingJobFile {
  std::string path;
  uint64_t size_bytes{};
};
std::mutex pending_files_mutex;
std::map<std::string, PendingJobFile> pending_job_files;

const char* SHM_JOB_QUEUE    = "/cuopt_job_queue";
const char* SHM_RESULT_QUEUE = "/cuopt_result_queue";
const char* SHM_CONTROL      = "/cuopt_control";

const std::string LOG_DIR = "/tmp/cuopt_logs";
inline std::string get_log_file_path(const std::string& job_id)
{
  return LOG_DIR + "/job_" + job_id + ".log";
}

const std::string UPLOAD_DIR = "/tmp/cuopt_uploads";
inline std::string get_upload_file_path(const std::string& upload_id)
{
  return UPLOAD_DIR + "/upload_" + upload_id + ".bin";
}

// ============================================================================
// Signal handling
// ============================================================================

void signal_handler(int signal)
{
  if (signal == SIGINT || signal == SIGTERM) {
    std::cout << "\n[gRPC Server] Received shutdown signal\n";
    keep_running = false;
    if (shm_ctrl) { shm_ctrl->shutdown_requested = true; }
    result_cv.notify_all();
  }
}

// ============================================================================
// Forward declarations
// ============================================================================

std::string generate_job_id();
void ensure_log_dir_exists();
void delete_log_file(const std::string& job_id);
void ensure_upload_dir_exists();
void delete_upload_file(const std::string& upload_id);
void cleanup_shared_memory();
void spawn_workers();
void wait_for_workers();
void worker_monitor_thread();
void result_retrieval_thread();
void incumbent_retrieval_thread();

// Pipe and shared memory functions
static bool write_to_pipe(int fd, const void* data, size_t size);
static bool read_from_pipe(int fd, void* data, size_t size, int timeout_ms = 120000);
static bool send_job_data_pipe(int worker_idx, const std::vector<uint8_t>& data);
static bool send_job_data_pipe_file(int worker_idx,
                                    const std::string& path,
                                    uint64_t expected_size);
static bool recv_job_data_pipe(int fd, uint64_t expected_size, std::vector<uint8_t>& data);
static bool send_result_pipe(int fd, const std::vector<uint8_t>& data);
static bool send_incumbent_pipe(int fd, const std::vector<uint8_t>& data);
static bool recv_incumbent_pipe(int fd, std::vector<uint8_t>& data);
static bool recv_result_pipe(int worker_idx, uint64_t expected_size, std::vector<uint8_t>& data);
static std::string create_job_shm(const std::string& job_id,
                                  const std::vector<uint8_t>& data,
                                  const char* suffix);
static bool read_job_shm(const char* shm_name, size_t data_size, std::vector<uint8_t>& data);
static std::string write_result_shm(const std::string& job_id, const std::vector<uint8_t>& data);
static void cleanup_job_shm(const char* shm_name);

constexpr int64_t kMiB = 1024LL * 1024;
constexpr int64_t kGiB = 1024LL * 1024 * 1024;

class IncumbentPipeCallback : public cuopt::internals::get_solution_callback_t {
 public:
  IncumbentPipeCallback(std::string job_id, int fd) : job_id_(std::move(job_id)), fd_(fd) {}

  void get_solution(void* data, void* objective_value) override
  {
    if (fd_ < 0 || n_variables == 0) { return; }

    double objective = 0.0;
    std::vector<double> assignment;
    assignment.resize(n_variables);

    if (isFloat) {
      std::vector<float> tmp(n_variables);
      if (cudaMemcpy(tmp.data(), data, n_variables * sizeof(float), cudaMemcpyDeviceToHost) !=
          cudaSuccess) {
        return;
      }
      for (size_t i = 0; i < n_variables; ++i) {
        assignment[i] = static_cast<double>(tmp[i]);
      }
      float obj = 0.0f;
      if (cudaMemcpy(&obj, objective_value, sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) {
        return;
      }
      objective = static_cast<double>(obj);
    } else {
      if (cudaMemcpy(
            assignment.data(), data, n_variables * sizeof(double), cudaMemcpyDeviceToHost) !=
          cudaSuccess) {
        return;
      }
      double obj = 0.0;
      if (cudaMemcpy(&obj, objective_value, sizeof(double), cudaMemcpyDeviceToHost) !=
          cudaSuccess) {
        return;
      }
      objective = obj;
    }

    cuopt::remote::Incumbent msg;
    msg.set_job_id(job_id_);
    msg.set_objective(objective);
    for (double v : assignment) {
      msg.add_assignment(v);
    }

    std::vector<uint8_t> buffer(msg.ByteSizeLong());
    if (!msg.SerializeToArray(buffer.data(), buffer.size())) { return; }
    std::cout << "[Worker] Incumbent callback job_id=" << job_id_ << " obj=" << objective
              << " vars=" << assignment.size() << "\n";
    std::cout.flush();
    send_incumbent_pipe(fd_, buffer);
  }

 private:
  std::string job_id_;
  int fd_;
};

static void store_simple_result(const std::string& job_id,
                                int worker_id,
                                int status,
                                const char* error_message)
{
  for (size_t i = 0; i < MAX_RESULTS; ++i) {
    if (!result_queue[i].ready) {
      copy_cstr(result_queue[i].job_id, job_id);
      result_queue[i].status           = status;
      result_queue[i].data_size        = 0;
      result_queue[i].shm_data_name[0] = '\0';
      result_queue[i].worker_index     = worker_id;
      copy_cstr(result_queue[i].error_message, error_message);
      result_queue[i].error_message[sizeof(result_queue[i].error_message) - 1] = '\0';
      result_queue[i].retrieved                                                = false;
      result_queue[i].ready                                                    = true;
      break;
    }
  }
}

// ============================================================================
// Worker Infrastructure (shared with the remote solve server implementation)
// ============================================================================
void cleanup_shared_memory()
{
  if (job_queue) {
    munmap(job_queue, sizeof(JobQueueEntry) * MAX_JOBS);
    shm_unlink(SHM_JOB_QUEUE);
  }
  if (result_queue) {
    munmap(result_queue, sizeof(ResultQueueEntry) * MAX_RESULTS);
    shm_unlink(SHM_RESULT_QUEUE);
  }
  if (shm_ctrl) {
    munmap(shm_ctrl, sizeof(SharedMemoryControl));
    shm_unlink(SHM_CONTROL);
  }
}

void worker_process(int worker_id)
{
  std::cout << "[Worker " << worker_id << "] Started (PID: " << getpid() << ")\n";

  // Increment active worker count
  shm_ctrl->active_workers++;

  // NOTE: We create raft::handle_t AFTER stdout redirect (per-job) so that
  // CUDA logging uses the redirected output streams.

  // Get serializer
  auto serializer = get_serializer<int, double>();

  while (!shm_ctrl->shutdown_requested) {
    // Find a job to process
    int job_slot = -1;
    for (size_t i = 0; i < MAX_JOBS; ++i) {
      if (job_queue[i].ready && !job_queue[i].claimed) {
        // Try to claim this job atomically
        bool expected = false;
        if (job_queue[i].claimed.compare_exchange_strong(expected, true)) {
          job_queue[i].worker_pid   = getpid();   // Record our PID
          job_queue[i].worker_index = worker_id;  // Record worker index for pipe mode
          job_slot                  = i;
          break;
        }
      }
    }

    if (job_slot < 0) {
      // No job available, sleep briefly
      usleep(10000);  // 10ms
      continue;
    }

    // Process the job
    JobQueueEntry& job = job_queue[job_slot];
    std::string job_id(job.job_id);
    bool is_mip = (job.problem_type == 1);

    // Check if job was cancelled before we start processing
    if (job.cancelled) {
      std::cout << "[Worker " << worker_id << "] Job cancelled before processing: " << job_id
                << "\n";
      std::cout.flush();

      // Cleanup job input shm (shm mode only)
      if (!config.use_pipes) { cleanup_job_shm(job.shm_data_name); }

      // Store cancelled result in result queue
      store_simple_result(job_id, worker_id, 2, "Job was cancelled");

      // Clear job slot (don't exit/restart worker)
      job.worker_pid   = 0;
      job.worker_index = -1;
      job.data_sent    = false;
      job.ready        = false;
      job.claimed      = false;
      job.cancelled    = false;
      continue;  // Go back to waiting for next job
    }

    std::cout << "[Worker " << worker_id << "] Processing job: " << job_id
              << " (type: " << (is_mip ? "MIP" : "LP") << ")\n";
    std::cout.flush();

    std::string log_file = get_log_file_path(job_id);

    // Create RAFT handle before calling solver
    std::cout << "[Worker] Creating raft::handle_t...\n" << std::flush;

    raft::handle_t handle;

    std::cout << "[Worker] Handle created, starting solve...\n" << std::flush;

    // Read problem data (pipe mode or shm mode)
    std::vector<uint8_t> request_data;
    bool read_success = false;
    if (config.use_pipes) {
      // Pipe mode: read from pipe (blocks until server writes data)
      // No need to wait for data_sent flag - pipe read naturally blocks
      int read_fd  = worker_pipes[worker_id].worker_read_fd;
      read_success = recv_job_data_pipe(read_fd, job.data_size, request_data);
      if (!read_success) {
        std::cerr << "[Worker " << worker_id << "] Failed to read job data from pipe\n";
      }
    } else {
      // SHM mode: read from shared memory
      read_success = read_job_shm(job.shm_data_name, job.data_size, request_data);
      if (!read_success) {
        std::cerr << "[Worker " << worker_id << "] Failed to read job data from shm\n";
      }
      // Cleanup job input shm now that we've read it
      cleanup_job_shm(job.shm_data_name);
    }

    if (!read_success) {
      // Store error result
      store_simple_result(job_id, worker_id, 1, "Failed to read job data");
      // Clear job slot
      job.worker_pid   = 0;
      job.worker_index = -1;
      job.data_sent    = false;
      job.ready        = false;
      job.claimed      = false;
      continue;
    }

    std::vector<uint8_t> result_data;
    std::string error_message;
    bool success = false;

    try {
      cuopt::mps_parser::mps_data_model_t<int, double> mps_data;
      if (is_mip) {
        mip_solver_settings_t<int, double> settings;
        std::unique_ptr<IncumbentPipeCallback> incumbent_cb;
        settings.log_file       = log_file;
        settings.log_to_console = config.log_to_console;

        if (serializer->deserialize_mip_request(request_data, mps_data, settings)) {
          bool enable_incumbents = true;
          cuopt::remote::SolveMIPRequest mip_request;
          if (mip_request.ParseFromArray(request_data.data(), request_data.size()) &&
              mip_request.has_enable_incumbents()) {
            enable_incumbents = mip_request.enable_incumbents();
          }
          if (enable_incumbents) {
            incumbent_cb = std::make_unique<IncumbentPipeCallback>(
              job_id, worker_pipes[worker_id].worker_incumbent_write_fd);
            settings.set_mip_callback(incumbent_cb.get());
            std::cout << "[Worker] Registered incumbent callback for job_id=" << job_id
                      << " callbacks=" << settings.get_mip_callbacks().size() << "\n";
            std::cout.flush();
          } else {
            std::cout << "[Worker] Skipping incumbent callback for job_id=" << job_id << "\n";
            std::cout.flush();
          }
          std::cout << "[Worker] Calling solve_mip...\n" << std::flush;
          auto solution = solve_mip(&handle, mps_data, settings);
          std::cout << "[Worker] solve_mip done\n" << std::flush;
          solution.to_host(handle.get_stream());
          result_data = serializer->serialize_mip_solution(solution);
          success     = true;
        } else {
          error_message = "Failed to deserialize MIP request";
        }
      } else {
        pdlp_solver_settings_t<int, double> settings;
        settings.log_file       = log_file;
        settings.log_to_console = config.log_to_console;

        if (serializer->deserialize_lp_request(request_data, mps_data, settings)) {
          std::cout << "[Worker] Calling solve_lp...\n" << std::flush;
          auto solution = solve_lp(&handle, mps_data, settings);
          std::cout << "[Worker] solve_lp done\n" << std::flush;
          solution.to_host(handle.get_stream());
          result_data = serializer->serialize_lp_solution(solution);
          success     = true;
        } else {
          error_message = "Failed to deserialize LP request";
        }
      }
    } catch (const std::exception& e) {
      error_message = std::string("Exception: ") + e.what();
    }

    // Store result (pipe mode: write to pipe, shm mode: write to shared memory)
    if (config.use_pipes) {
      // PIPE MODE: Set result_queue metadata FIRST, THEN write to pipe.
      // This avoids deadlock: the main thread's result_retrieval_thread
      // needs to see ready=true before it will read from the pipe,
      // but if we write to pipe first with a large result, we'll block
      // waiting for the reader that will never come.

      // Find a free result slot and populate metadata
      int result_slot = -1;
      for (size_t i = 0; i < MAX_RESULTS; ++i) {
        if (!result_queue[i].ready) {
          result_slot              = i;
          ResultQueueEntry& result = result_queue[i];
          copy_cstr(result.job_id, job_id);
          result.status           = success ? 0 : 1;
          result.data_size        = success ? result_data.size() : 0;
          result.shm_data_name[0] = '\0';  // Not used in pipe mode
          result.worker_index     = worker_id;
          if (!success) { copy_cstr(result.error_message, error_message); }
          result.retrieved = false;
          // Set ready=true BEFORE writing to pipe so reader thread starts reading
          // This prevents deadlock with large results that exceed pipe buffer size
          result.ready = true;
          if (config.verbose) {
            std::cout << "[Worker " << worker_id << "] Enqueued result metadata for job " << job_id
                      << " in result_slot=" << result_slot << " status=" << result.status
                      << " data_size=" << result.data_size << "\n";
            std::cout.flush();
          }
          break;
        }
      }

      // Now write result data to pipe (reader thread should be ready to receive)
      if (success && !result_data.empty() && result_slot >= 0) {
        int write_fd = worker_pipes[worker_id].worker_write_fd;
        if (config.verbose) {
          std::cout << "[Worker " << worker_id << "] Writing " << result_data.size()
                    << " bytes of result payload to pipe for job " << job_id << "\n";
          std::cout.flush();
        }
        bool write_success = send_result_pipe(write_fd, result_data);
        if (!write_success) {
          std::cerr << "[Worker " << worker_id << "] Failed to write result to pipe\n";
          std::cerr.flush();
          // Mark as failed in result queue
          result_queue[result_slot].status = 1;
          copy_cstr(result_queue[result_slot].error_message, "Failed to write result to pipe");
        } else if (config.verbose) {
          std::cout << "[Worker " << worker_id << "] Finished writing result payload for job "
                    << job_id << "\n";
          std::cout.flush();
        }
      } else if (config.verbose) {
        std::cout << "[Worker " << worker_id << "] No result payload write needed for job "
                  << job_id << " (success=" << success << ", result_slot=" << result_slot
                  << ", payload_bytes=" << result_data.size() << ")\n";
        std::cout.flush();
      }
    } else {
      // SHM mode: store result in shared memory
      for (size_t i = 0; i < MAX_RESULTS; ++i) {
        if (!result_queue[i].ready) {
          ResultQueueEntry& result = result_queue[i];
          copy_cstr(result.job_id, job_id);
          result.status       = success ? 0 : 1;
          result.worker_index = worker_id;
          if (success && !result_data.empty()) {
            // Create per-result shared memory
            std::string shm_name = write_result_shm(job_id, result_data);
            if (shm_name.empty()) {
              // Failed to create shm - report error
              result.status           = 1;
              result.data_size        = 0;
              result.shm_data_name[0] = '\0';
              copy_cstr(result.error_message, "Failed to create shared memory for result");
            } else {
              result.data_size = result_data.size();
              copy_cstr(result.shm_data_name, shm_name);
            }
          } else if (!success) {
            copy_cstr(result.error_message, error_message);
            result.data_size        = 0;
            result.shm_data_name[0] = '\0';
          } else {
            result.data_size        = 0;
            result.shm_data_name[0] = '\0';
          }
          result.retrieved = false;
          result.ready     = true;  // Mark as ready last
          break;
        }
      }
    }

    // Clear job slot
    job.worker_pid   = 0;
    job.worker_index = -1;
    job.data_sent    = false;
    job.ready        = false;
    job.claimed      = false;
    job.cancelled    = false;

    std::cout << "[Worker " << worker_id << "] Completed job: " << job_id
              << " (success: " << success << ")\n";
  }

  shm_ctrl->active_workers--;
  std::cout << "[Worker " << worker_id << "] Stopped\n";
  _exit(0);
}

// Create pipes for a worker (incumbent always, data/result in pipe mode)
bool create_worker_pipes(int worker_id)
{
  // Ensure worker_pipes has enough slots
  while (static_cast<int>(worker_pipes.size()) <= worker_id) {
    worker_pipes.push_back({-1, -1, -1, -1, -1, -1});
  }

  WorkerPipes& wp = worker_pipes[worker_id];

  if (config.use_pipes) {
    // Create pipe for server -> worker data
    int input_pipe[2];
    if (pipe(input_pipe) < 0) {
      std::cerr << "[Server] Failed to create input pipe for worker " << worker_id << "\n";
      return false;
    }
    wp.worker_read_fd = input_pipe[0];  // Worker reads from this
    wp.to_worker_fd   = input_pipe[1];  // Server writes to this

    // Create pipe for worker -> server results
    int output_pipe[2];
    if (pipe(output_pipe) < 0) {
      std::cerr << "[Server] Failed to create output pipe for worker " << worker_id << "\n";
      close(input_pipe[0]);
      close(input_pipe[1]);
      return false;
    }
    wp.from_worker_fd  = output_pipe[0];  // Server reads from this
    wp.worker_write_fd = output_pipe[1];  // Worker writes to this
  }

  int incumbent_pipe[2];
  if (pipe(incumbent_pipe) < 0) {
    std::cerr << "[Server] Failed to create incumbent pipe for worker " << worker_id << "\n";
    if (config.use_pipes) {
      if (wp.worker_read_fd >= 0) close(wp.worker_read_fd);
      if (wp.to_worker_fd >= 0) close(wp.to_worker_fd);
      if (wp.from_worker_fd >= 0) close(wp.from_worker_fd);
      if (wp.worker_write_fd >= 0) close(wp.worker_write_fd);
      wp.worker_read_fd  = -1;
      wp.to_worker_fd    = -1;
      wp.from_worker_fd  = -1;
      wp.worker_write_fd = -1;
    }
    return false;
  }
  wp.incumbent_from_worker_fd  = incumbent_pipe[0];  // Server reads from this
  wp.worker_incumbent_write_fd = incumbent_pipe[1];  // Worker writes to this

  return true;
}

// Close server-side pipe ends for a worker (called when restarting)
void close_worker_pipes_server(int worker_id)
{
  if (worker_id < 0 || worker_id >= static_cast<int>(worker_pipes.size())) return;

  WorkerPipes& wp = worker_pipes[worker_id];
  if (config.use_pipes) {
    if (wp.to_worker_fd >= 0) {
      close(wp.to_worker_fd);
      wp.to_worker_fd = -1;
    }
    if (wp.from_worker_fd >= 0) {
      close(wp.from_worker_fd);
      wp.from_worker_fd = -1;
    }
  }
  if (wp.incumbent_from_worker_fd >= 0) {
    close(wp.incumbent_from_worker_fd);
    wp.incumbent_from_worker_fd = -1;
  }
}

// Close worker-side pipe ends in parent after fork
void close_worker_pipes_child_ends(int worker_id)
{
  if (worker_id < 0 || worker_id >= static_cast<int>(worker_pipes.size())) return;

  WorkerPipes& wp = worker_pipes[worker_id];
  if (config.use_pipes) {
    if (wp.worker_read_fd >= 0) {
      close(wp.worker_read_fd);
      wp.worker_read_fd = -1;
    }
    if (wp.worker_write_fd >= 0) {
      close(wp.worker_write_fd);
      wp.worker_write_fd = -1;
    }
  }
  if (wp.worker_incumbent_write_fd >= 0) {
    close(wp.worker_incumbent_write_fd);
    wp.worker_incumbent_write_fd = -1;
  }
}

pid_t spawn_worker(int worker_id, bool is_replacement)
{
  if (is_replacement) { close_worker_pipes_server(worker_id); }

  if (!create_worker_pipes(worker_id)) {
    std::cerr << "[Server] Failed to create pipes for "
              << (is_replacement ? "replacement worker " : "worker ") << worker_id << "\n";
    return -1;
  }

  pid_t pid = fork();
  if (pid < 0) {
    std::cerr << "[Server] Failed to fork " << (is_replacement ? "replacement worker " : "worker ")
              << worker_id << "\n";
    close_worker_pipes_server(worker_id);
    return -1;
  } else if (pid == 0) {
    // Child process
    if (config.use_pipes) {
      // Close all other workers' pipe fds
      for (int j = 0; j < static_cast<int>(worker_pipes.size()); ++j) {
        if (j != worker_id) {
          if (worker_pipes[j].worker_read_fd >= 0) close(worker_pipes[j].worker_read_fd);
          if (worker_pipes[j].worker_write_fd >= 0) close(worker_pipes[j].worker_write_fd);
          if (worker_pipes[j].to_worker_fd >= 0) close(worker_pipes[j].to_worker_fd);
          if (worker_pipes[j].from_worker_fd >= 0) close(worker_pipes[j].from_worker_fd);
          if (worker_pipes[j].incumbent_from_worker_fd >= 0) {
            close(worker_pipes[j].incumbent_from_worker_fd);
          }
          if (worker_pipes[j].worker_incumbent_write_fd >= 0) {
            close(worker_pipes[j].worker_incumbent_write_fd);
          }
        }
      }
      // Close server ends of our pipes
      close(worker_pipes[worker_id].to_worker_fd);
      close(worker_pipes[worker_id].from_worker_fd);
    }
    if (worker_pipes[worker_id].incumbent_from_worker_fd >= 0) {
      close(worker_pipes[worker_id].incumbent_from_worker_fd);
      worker_pipes[worker_id].incumbent_from_worker_fd = -1;
    }
    worker_process(worker_id);
    _exit(0);  // Should not reach here
  }

  // Parent: close worker ends of new pipes
  close_worker_pipes_child_ends(worker_id);
  return pid;
}

void spawn_workers()
{
  for (int i = 0; i < config.num_workers; ++i) {
    pid_t pid = spawn_worker(i, false);
    if (pid < 0) { continue; }
    worker_pids.push_back(pid);
  }
}

void wait_for_workers()
{
  for (pid_t pid : worker_pids) {
    int status;
    waitpid(pid, &status, 0);
  }
  worker_pids.clear();
}

// Spawn a single replacement worker and return its PID
pid_t spawn_single_worker(int worker_id) { return spawn_worker(worker_id, true); }

// Mark jobs being processed by a dead worker as failed (or cancelled if it was cancelled)
void mark_worker_jobs_failed(pid_t dead_worker_pid)
{
  for (size_t i = 0; i < MAX_JOBS; ++i) {
    if (job_queue[i].ready && job_queue[i].claimed && job_queue[i].worker_pid == dead_worker_pid) {
      std::string job_id(job_queue[i].job_id);
      bool was_cancelled = job_queue[i].cancelled;

      if (was_cancelled) {
        std::cerr << "[Server] Worker " << dead_worker_pid
                  << " killed for cancelled job: " << job_id << "\n";
      } else {
        std::cerr << "[Server] Worker " << dead_worker_pid
                  << " died while processing job: " << job_id << "\n";
      }

      // Cleanup job data
      if (config.use_pipes) {
        // Pipe mode: remove from pending data if not yet sent
        {
          std::lock_guard<std::mutex> lock(pending_data_mutex);
          pending_job_data.erase(job_id);
        }
        {
          std::lock_guard<std::mutex> lock(pending_files_mutex);
          auto itf = pending_job_files.find(job_id);
          if (itf != pending_job_files.end()) {
            unlink(itf->second.path.c_str());
            pending_job_files.erase(itf);
          }
        }
      } else {
        // SHM mode: cleanup job input shm (worker may not have done it)
        cleanup_job_shm(job_queue[i].shm_data_name);
      }

      // Store result in result queue (cancelled or failed)
      for (size_t j = 0; j < MAX_RESULTS; ++j) {
        if (!result_queue[j].ready) {
          copy_cstr(result_queue[j].job_id, job_id);
          result_queue[j].status           = was_cancelled ? 2 : 1;  // 2=cancelled, 1=error
          result_queue[j].data_size        = 0;
          result_queue[j].shm_data_name[0] = '\0';
          result_queue[j].worker_index     = -1;
          copy_cstr(result_queue[j].error_message,
                    was_cancelled ? "Job was cancelled" : "Worker process died unexpectedly");
          result_queue[j].retrieved = false;
          result_queue[j].ready     = true;
          break;
        }
      }

      // Clear the job slot
      job_queue[i].worker_pid   = 0;
      job_queue[i].worker_index = -1;
      job_queue[i].data_sent    = false;
      job_queue[i].ready        = false;
      job_queue[i].claimed      = false;
      job_queue[i].cancelled    = false;

      // Update job tracker
      {
        std::lock_guard<std::mutex> lock(tracker_mutex);
        auto it = job_tracker.find(job_id);
        if (it != job_tracker.end()) {
          if (was_cancelled) {
            it->second.status        = JobStatus::CANCELLED;
            it->second.error_message = "Job was cancelled";
          } else {
            it->second.status        = JobStatus::FAILED;
            it->second.error_message = "Worker process died unexpectedly";
          }
        }
      }
    }
  }
}

// Worker monitor thread - detects dead workers and restarts them
void worker_monitor_thread()
{
  std::cout << "[Server] Worker monitor thread started\n";
  std::cout.flush();

  while (keep_running) {
    // Check all worker PIDs for dead workers
    for (size_t i = 0; i < worker_pids.size(); ++i) {
      pid_t pid = worker_pids[i];
      if (pid <= 0) continue;

      int status;
      pid_t result = waitpid(pid, &status, WNOHANG);

      if (result == pid) {
        // Worker has exited
        int exit_code  = WIFEXITED(status) ? WEXITSTATUS(status) : -1;
        bool signaled  = WIFSIGNALED(status);
        int signal_num = signaled ? WTERMSIG(status) : 0;

        if (signaled) {
          std::cerr << "[Server] Worker " << pid << " killed by signal " << signal_num << "\n";
          std::cerr.flush();
        } else if (exit_code != 0) {
          std::cerr << "[Server] Worker " << pid << " exited with code " << exit_code << "\n";
          std::cerr.flush();
        } else {
          // Clean exit during shutdown - don't restart
          if (shm_ctrl && shm_ctrl->shutdown_requested) {
            worker_pids[i] = 0;
            continue;
          }
          std::cerr << "[Server] Worker " << pid << " exited unexpectedly\n";
          std::cerr.flush();
        }

        // Mark any jobs this worker was processing as failed
        mark_worker_jobs_failed(pid);

        // Spawn replacement worker (unless shutting down)
        if (keep_running && shm_ctrl && !shm_ctrl->shutdown_requested) {
          pid_t new_pid = spawn_single_worker(static_cast<int>(i));
          if (new_pid > 0) {
            worker_pids[i] = new_pid;
            std::cout << "[Server] Restarted worker " << i << " with PID " << new_pid << "\n";
            std::cout.flush();
          } else {
            worker_pids[i] = 0;  // Failed to restart
          }
        } else {
          worker_pids[i] = 0;
        }
      }
    }

    // Check every 100ms
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
  }

  std::cout << "[Server] Worker monitor thread stopped\n";
  std::cout.flush();
}

void result_retrieval_thread()
{
  std::cout << "[Server] Result retrieval thread started\n";
  std::cout.flush();

  while (keep_running) {
    bool found = false;

    // PIPE MODE: Check for jobs that need data sent to workers
    if (config.use_pipes) {
      for (size_t i = 0; i < MAX_JOBS; ++i) {
        if (job_queue[i].ready && job_queue[i].claimed && !job_queue[i].data_sent &&
            !job_queue[i].cancelled) {
          std::string job_id(job_queue[i].job_id);
          int worker_idx = job_queue[i].worker_index;

          std::cout << "[DEBUG RRT] Found job needing data: " << job_id
                    << " worker_idx=" << worker_idx << std::endl;
          std::cout.flush();

          if (worker_idx >= 0) {
            // Prefer file-backed payloads (streaming upload).
            PendingJobFile pending_file;
            bool have_file = false;
            {
              std::lock_guard<std::mutex> lock(pending_files_mutex);
              auto itf = pending_job_files.find(job_id);
              if (itf != pending_job_files.end()) {
                pending_file = itf->second;
                pending_job_files.erase(itf);
                have_file = true;
              }
            }

            if (have_file) {
              if (config.verbose) {
                std::cout << "[Server] Sending file-backed payload to worker " << worker_idx
                          << " for job " << job_id << " size=" << pending_file.size_bytes << "\n";
                std::cout.flush();
              }
              bool ok =
                send_job_data_pipe_file(worker_idx, pending_file.path, pending_file.size_bytes);
              unlink(pending_file.path.c_str());  // best-effort cleanup
              if (ok) {
                job_queue[i].data_sent = true;
              } else {
                std::cerr << "[Server] Failed to send file-backed payload to worker " << worker_idx
                          << "\n";
                job_queue[i].cancelled = true;
              }
              found = true;
              continue;
            }

            // Get pending job data
            std::vector<uint8_t> job_data;
            {
              std::lock_guard<std::mutex> lock(pending_data_mutex);
              std::cout << "[DEBUG RRT] Looking for pending data, pending_job_data size="
                        << pending_job_data.size() << std::endl;
              std::cout.flush();
              auto it = pending_job_data.find(job_id);
              if (it != pending_job_data.end()) {
                job_data = std::move(it->second);
                pending_job_data.erase(it);
                std::cout << "[DEBUG RRT] Found and moved data for job " << job_id
                          << ", size=" << job_data.size() << std::endl;
                std::cout.flush();
              } else {
                std::cout << "[DEBUG RRT] NOT FOUND in pending_job_data for job_id=" << job_id
                          << std::endl;
                std::cout.flush();
              }
            }

            if (!job_data.empty()) {
              // Send data to worker's pipe
              if (send_job_data_pipe(worker_idx, job_data)) {
                job_queue[i].data_sent = true;
                if (config.verbose) {
                  std::cout << "[Server] Sent " << job_data.size() << " bytes to worker "
                            << worker_idx << " for job " << job_id << "\n";
                }
              } else {
                std::cerr << "[Server] Failed to send job data to worker " << worker_idx << "\n";
                // Mark job as failed
                job_queue[i].cancelled = true;
              }
              found = true;
            }
          }
        }
      }
    }

    // Check for completed results
    for (size_t i = 0; i < MAX_RESULTS; ++i) {
      if (result_queue[i].ready && !result_queue[i].retrieved) {
        std::string job_id(result_queue[i].job_id);
        uint32_t result_status = result_queue[i].status;
        bool success           = (result_status == 0);
        bool cancelled         = (result_status == 2);
        int worker_idx         = result_queue[i].worker_index;
        if (config.verbose) {
          std::cout << "[Server] Detected ready result_slot=" << i << " for job " << job_id
                    << " status=" << result_status << " data_size=" << result_queue[i].data_size
                    << " worker_idx=" << worker_idx << "\n";
          std::cout.flush();
        }

        std::vector<uint8_t> result_data;
        std::string error_message;

        if (success && result_queue[i].data_size > 0) {
          if (config.use_pipes) {
            // Pipe mode: read result from worker's output pipe
            if (config.verbose) {
              std::cout << "[Server] Reading " << result_queue[i].data_size
                        << " bytes from worker pipe for job " << job_id << "\n";
              std::cout.flush();
            }
            if (!recv_result_pipe(worker_idx, result_queue[i].data_size, result_data)) {
              error_message = "Failed to read result data from pipe";
              success       = false;
            }
          } else {
            // SHM mode: read from shared memory
            if (!read_job_shm(
                  result_queue[i].shm_data_name, result_queue[i].data_size, result_data)) {
              error_message = "Failed to read result data from shared memory";
              success       = false;
            }
            // Cleanup result shm after reading
            cleanup_job_shm(result_queue[i].shm_data_name);
          }
        } else if (!success) {
          error_message = result_queue[i].error_message;
        }

        // Check if there's a blocking waiter
        {
          std::lock_guard<std::mutex> lock(waiters_mutex);
          auto wit = waiting_threads.find(job_id);
          if (wit != waiting_threads.end()) {
            // Wake up all waiting threads sharing this waiter
            auto waiter = wit->second;
            {
              std::lock_guard<std::mutex> waiter_lock(waiter->mutex);
              waiter->result_data   = std::move(result_data);
              waiter->error_message = error_message;
              waiter->success       = success;
              waiter->ready         = true;
            }
            waiter->cv.notify_all();
            waiting_threads.erase(wit);
          }
        }

        // Update job tracker
        {
          std::lock_guard<std::mutex> lock(tracker_mutex);
          auto it = job_tracker.find(job_id);
          if (it != job_tracker.end()) {
            if (success) {
              it->second.status      = JobStatus::COMPLETED;
              it->second.result_data = result_data;
              if (config.verbose) {
                std::cout << "[Server] Marked job COMPLETED in job_tracker: " << job_id
                          << " result_bytes=" << result_data.size() << "\n";
                std::cout.flush();
              }
            } else if (cancelled) {
              it->second.status        = JobStatus::CANCELLED;
              it->second.error_message = error_message;
              if (config.verbose) {
                std::cout << "[Server] Marked job CANCELLED in job_tracker: " << job_id
                          << " msg=" << error_message << "\n";
                std::cout.flush();
              }
            } else {
              it->second.status        = JobStatus::FAILED;
              it->second.error_message = error_message;
              if (config.verbose) {
                std::cout << "[Server] Marked job FAILED in job_tracker: " << job_id
                          << " msg=" << error_message << "\n";
                std::cout.flush();
              }
            }
          } else if (config.verbose) {
            std::cout << "[Server] WARNING: result for unknown job_id (not in job_tracker): "
                      << job_id << "\n";
            std::cout.flush();
          }
        }

        result_queue[i].retrieved    = true;
        result_queue[i].worker_index = -1;
        result_queue[i].ready        = false;  // Free slot
        found                        = true;
      }
    }

    if (!found) {
      usleep(10000);  // 10ms
    }

    result_cv.notify_all();
  }

  std::cout << "[Server] Result retrieval thread stopped\n";
  std::cout.flush();
}

void incumbent_retrieval_thread()
{
  std::cout << "[Server] Incumbent retrieval thread started\n";
  std::cout.flush();

  while (keep_running) {
    std::vector<pollfd> pfds;
    pfds.reserve(worker_pipes.size());
    for (const auto& wp : worker_pipes) {
      if (wp.incumbent_from_worker_fd >= 0) {
        pollfd pfd;
        pfd.fd      = wp.incumbent_from_worker_fd;
        pfd.events  = POLLIN;
        pfd.revents = 0;
        pfds.push_back(pfd);
      }
    }

    if (pfds.empty()) {
      std::this_thread::sleep_for(std::chrono::milliseconds(100));
      continue;
    }

    int poll_result = poll(pfds.data(), pfds.size(), 100);
    if (poll_result < 0) {
      if (errno == EINTR) continue;
      std::cerr << "[Server] poll() failed in incumbent thread: " << strerror(errno) << "\n";
      std::this_thread::sleep_for(std::chrono::milliseconds(100));
      continue;
    }
    if (poll_result == 0) { continue; }

    for (const auto& pfd : pfds) {
      if (!(pfd.revents & POLLIN)) { continue; }
      std::vector<uint8_t> data;
      if (!recv_incumbent_pipe(pfd.fd, data)) { continue; }
      if (data.empty()) { continue; }

      cuopt::remote::Incumbent incumbent_msg;
      if (!incumbent_msg.ParseFromArray(data.data(), data.size())) {
        std::cerr << "[Server] Failed to parse incumbent payload\n";
        continue;
      }

      const std::string job_id = incumbent_msg.job_id();
      if (job_id.empty()) { continue; }

      IncumbentEntry entry;
      entry.objective = incumbent_msg.objective();
      entry.assignment.reserve(incumbent_msg.assignment_size());
      for (int i = 0; i < incumbent_msg.assignment_size(); ++i) {
        entry.assignment.push_back(incumbent_msg.assignment(i));
      }

      {
        std::lock_guard<std::mutex> lock(tracker_mutex);
        auto it = job_tracker.find(job_id);
        if (it != job_tracker.end()) {
          it->second.incumbents.push_back(std::move(entry));
          std::cout << "[Server] Stored incumbent job_id=" << job_id
                    << " idx=" << (it->second.incumbents.size() - 1)
                    << " obj=" << incumbent_msg.objective()
                    << " vars=" << incumbent_msg.assignment_size() << "\n";
          std::cout.flush();
        }
      }
    }
  }

  std::cout << "[Server] Incumbent retrieval thread stopped\n";
  std::cout.flush();
}

static std::string create_job_shm(const std::string& job_id,
                                  const std::vector<uint8_t>& data,
                                  const char* prefix)
{
  std::string shm_name = std::string("/cuopt_") + prefix + "_" + job_id;

  int fd = shm_open(shm_name.c_str(), O_CREAT | O_RDWR, 0666);
  if (fd < 0) {
    std::cerr << "[Server] Failed to create shm " << shm_name << ": " << strerror(errno) << "\n";
    return "";
  }

  if (ftruncate(fd, data.size()) < 0) {
    std::cerr << "[Server] Failed to size shm " << shm_name << ": " << strerror(errno) << "\n";
    close(fd);
    shm_unlink(shm_name.c_str());
    return "";
  }

  void* ptr = mmap(nullptr, data.size(), PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
  close(fd);

  if (ptr == MAP_FAILED) {
    std::cerr << "[Server] Failed to map shm " << shm_name << ": " << strerror(errno) << "\n";
    shm_unlink(shm_name.c_str());
    return "";
  }

  memcpy(ptr, data.data(), data.size());
  munmap(ptr, data.size());

  return shm_name;
}

static int64_t get_upload_mem_threshold_bytes()
{
  // Default to 1 GiB; set env CUOPT_GRPC_UPLOAD_MEM_THRESHOLD_BYTES to override.
  // 0 => always use file, -1 => always use memory (not recommended for huge uploads).
  const char* val = std::getenv("CUOPT_GRPC_UPLOAD_MEM_THRESHOLD_BYTES");
  if (!val || val[0] == '\0') { return kGiB; }
  try {
    return std::stoll(val);
  } catch (...) {
    return kGiB;
  }
}

static std::string read_file_to_string(const std::string& path)
{
  std::ifstream in(path, std::ios::in | std::ios::binary);
  if (!in.is_open()) { return ""; }
  std::ostringstream ss;
  ss << in.rdbuf();
  return ss.str();
}

// Read data from per-job shared memory segment
static bool read_job_shm(const char* shm_name, size_t data_size, std::vector<uint8_t>& data)
{
  int fd = shm_open(shm_name, O_RDONLY, 0666);
  if (fd < 0) {
    std::cerr << "[Worker] Failed to open shm " << shm_name << ": " << strerror(errno) << "\n";
    return false;
  }

  void* ptr = mmap(nullptr, data_size, PROT_READ, MAP_SHARED, fd, 0);
  close(fd);

  if (ptr == MAP_FAILED) {
    std::cerr << "[Worker] Failed to map shm " << shm_name << ": " << strerror(errno) << "\n";
    return false;
  }

  data.resize(data_size);
  memcpy(data.data(), ptr, data_size);
  munmap(ptr, data_size);

  return true;
}

// Write data to per-result shared memory segment
static std::string write_result_shm(const std::string& job_id, const std::vector<uint8_t>& data)
{
  return create_job_shm(job_id, data, "result");
}

// Cleanup per-job shared memory segment
static void cleanup_job_shm(const char* shm_name)
{
  if (shm_name[0] != '\0') { shm_unlink(shm_name); }
}

static bool write_to_pipe(int fd, const void* data, size_t size)
{
  const uint8_t* ptr = static_cast<const uint8_t*>(data);
  size_t remaining   = size;
  while (remaining > 0) {
    ssize_t written = ::write(fd, ptr, remaining);
    if (written <= 0) {
      if (errno == EINTR) continue;
      return false;
    }
    ptr += written;
    remaining -= written;
  }
  return true;
}

// Read all data from a pipe (handles partial reads) with timeout
// timeout_ms: milliseconds to wait for data (default 120000 = 2 minutes)
static bool read_from_pipe(int fd, void* data, size_t size, int timeout_ms)
{
  uint8_t* ptr     = static_cast<uint8_t*>(data);
  size_t remaining = size;
  while (remaining > 0) {
    // Use poll() to wait for data with timeout
    struct pollfd pfd;
    pfd.fd     = fd;
    pfd.events = POLLIN;

    int poll_result = poll(&pfd, 1, timeout_ms);
    if (poll_result < 0) {
      if (errno == EINTR) continue;
      std::cerr << "[Server] poll() failed on pipe: " << strerror(errno) << "\n";
      return false;
    }
    if (poll_result == 0) {
      std::cerr << "[Server] Timeout waiting for pipe data (waited " << timeout_ms << "ms)\n";
      return false;
    }
    if (pfd.revents & (POLLERR | POLLHUP | POLLNVAL)) {
      std::cerr << "[Server] Pipe error/hangup detected\n";
      return false;
    }

    ssize_t nread = ::read(fd, ptr, remaining);
    if (nread <= 0) {
      if (errno == EINTR) continue;
      if (nread == 0) { std::cerr << "[Server] Pipe EOF (writer closed)\n"; }
      return false;
    }
    ptr += nread;
    remaining -= nread;
  }
  return true;
}

// Send job data to worker via pipe (length-prefixed)
static bool send_job_data_pipe(int worker_idx, const std::vector<uint8_t>& data)
{
  if (worker_idx < 0 || worker_idx >= static_cast<int>(worker_pipes.size())) { return false; }
  int fd = worker_pipes[worker_idx].to_worker_fd;
  if (fd < 0) return false;

  // Send size first
  uint64_t size = data.size();
  if (!write_to_pipe(fd, &size, sizeof(size))) return false;
  // Send data
  if (size > 0 && !write_to_pipe(fd, data.data(), data.size())) return false;
  return true;
}

// Stream job data from a file to the worker via pipe (length-prefixed).
// This avoids holding the entire job payload in server memory.
static bool send_job_data_pipe_file(int worker_idx, const std::string& path, uint64_t expected_size)
{
  if (worker_idx < 0 || worker_idx >= static_cast<int>(worker_pipes.size())) { return false; }
  int pipe_fd = worker_pipes[worker_idx].to_worker_fd;
  if (pipe_fd < 0) return false;

  int file_fd = open(path.c_str(), O_RDONLY);
  if (file_fd < 0) {
    std::cerr << "[Server] Failed to open payload file: " << path << " err=" << strerror(errno)
              << "\n";
    return false;
  }

  // Send size first (worker validates it against expected_size in shared memory).
  uint64_t size = expected_size;
  if (!write_to_pipe(pipe_fd, &size, sizeof(size))) {
    close(file_fd);
    return false;
  }

  std::vector<uint8_t> buf(kMiB);  // 1 MiB
  uint64_t remaining = size;
  while (remaining > 0) {
    size_t to_read = buf.size();
    if (remaining < to_read) { to_read = static_cast<size_t>(remaining); }

    ssize_t nread = ::read(file_fd, buf.data(), to_read);
    if (nread < 0) {
      if (errno == EINTR) continue;
      std::cerr << "[Server] Failed reading payload file: " << path << " err=" << strerror(errno)
                << "\n";
      close(file_fd);
      return false;
    }
    if (nread == 0) {
      std::cerr << "[Server] Unexpected EOF reading payload file: " << path << "\n";
      close(file_fd);
      return false;
    }
    if (!write_to_pipe(pipe_fd, buf.data(), static_cast<size_t>(nread))) {
      close(file_fd);
      return false;
    }
    remaining -= static_cast<uint64_t>(nread);
  }

  close(file_fd);
  return true;
}

// Receive job data from pipe (length-prefixed) - called by worker
static bool recv_job_data_pipe(int fd, uint64_t expected_size, std::vector<uint8_t>& data)
{
  // Read size
  uint64_t size;
  if (!read_from_pipe(fd, &size, sizeof(size))) return false;
  if (size != expected_size) {
    std::cerr << "[Worker] Size mismatch: expected " << expected_size << ", got " << size << "\n";
    return false;
  }
  // Read data
  data.resize(size);
  if (size > 0 && !read_from_pipe(fd, data.data(), size)) return false;
  return true;
}

// Send result data to server via pipe (length-prefixed) - called by worker
static bool send_result_pipe(int fd, const std::vector<uint8_t>& data)
{
  // Send size first
  uint64_t size = data.size();
  if (!write_to_pipe(fd, &size, sizeof(size))) return false;
  // Send data
  if (size > 0 && !write_to_pipe(fd, data.data(), data.size())) return false;
  return true;
}

// Send incumbent data to server via pipe (length-prefixed) - called by worker
static bool send_incumbent_pipe(int fd, const std::vector<uint8_t>& data)
{
  uint64_t size = data.size();
  if (!write_to_pipe(fd, &size, sizeof(size))) return false;
  if (size > 0 && !write_to_pipe(fd, data.data(), data.size())) return false;
  return true;
}

// Receive incumbent data from worker via pipe (length-prefixed)
static bool recv_incumbent_pipe(int fd, std::vector<uint8_t>& data)
{
  uint64_t size;
  if (!read_from_pipe(fd, &size, sizeof(size))) return false;
  data.resize(size);
  if (size > 0 && !read_from_pipe(fd, data.data(), size)) return false;
  return true;
}

// Receive result data from worker via pipe (length-prefixed)
static bool recv_result_pipe(int worker_idx, uint64_t expected_size, std::vector<uint8_t>& data)
{
  if (worker_idx < 0 || worker_idx >= static_cast<int>(worker_pipes.size())) { return false; }
  int fd = worker_pipes[worker_idx].from_worker_fd;
  if (fd < 0) return false;

  // Read size
  uint64_t size;
  if (!read_from_pipe(fd, &size, sizeof(size))) return false;
  if (size != expected_size) {
    std::cerr << "[Server] Result size mismatch: expected " << expected_size << ", got " << size
              << "\n";
    return false;
  }
  // Read data
  data.resize(size);
  if (size > 0 && !read_from_pipe(fd, data.data(), size)) return false;
  return true;
}

// Submit a job asynchronously (returns job_id)
std::pair<bool, std::string> submit_job_async(const std::vector<uint8_t>& request_data, bool is_mip)
{
  std::string job_id = generate_job_id();

  std::string shm_name;
  if (config.use_pipes) {
    // Pipe mode: store data in pending map (will be sent when worker claims job)
    {
      std::lock_guard<std::mutex> lock(pending_data_mutex);
      pending_job_data[job_id] = request_data;
    }
  } else {
    // SHM mode: create per-job shared memory for problem data
    shm_name = create_job_shm(job_id, request_data, "job");
    if (shm_name.empty()) { return {false, "Failed to create shared memory for job data"}; }
  }

  // Find free job slot
  for (size_t i = 0; i < MAX_JOBS; ++i) {
    if (!job_queue[i].ready && !job_queue[i].claimed) {
      copy_cstr(job_queue[i].job_id, job_id);
      job_queue[i].problem_type = is_mip ? 1 : 0;
      job_queue[i].data_size    = request_data.size();
      if (!config.use_pipes) {
        copy_cstr(job_queue[i].shm_data_name, shm_name);
      } else {
        job_queue[i].shm_data_name[0] = '\0';
      }
      job_queue[i].worker_pid   = 0;
      job_queue[i].worker_index = -1;
      job_queue[i].data_sent    = false;
      job_queue[i].claimed      = false;
      job_queue[i].cancelled    = false;
      job_queue[i].ready        = true;  // Mark as ready last

      // Track job
      {
        std::lock_guard<std::mutex> lock(tracker_mutex);
        JobInfo info;
        info.job_id         = job_id;
        info.status         = JobStatus::QUEUED;
        info.submit_time    = std::chrono::steady_clock::now();
        info.is_mip         = is_mip;
        info.is_blocking    = false;
        job_tracker[job_id] = info;
      }

      if (config.verbose) { std::cout << "[Server] Job submitted (async): " << job_id << "\n"; }

      return {true, job_id};
    }
  }

  // No free slot - cleanup
  if (config.use_pipes) {
    std::lock_guard<std::mutex> lock(pending_data_mutex);
    pending_job_data.erase(job_id);
  } else {
    shm_unlink(shm_name.c_str());
  }
  return {false, "Job queue full"};
}

// Check job status
JobStatus check_job_status(const std::string& job_id, std::string& message)
{
  std::lock_guard<std::mutex> lock(tracker_mutex);
  auto it = job_tracker.find(job_id);

  if (it == job_tracker.end()) {
    message = "Job ID not found";
    return JobStatus::NOT_FOUND;
  }

  // If status is QUEUED, check if the job has been claimed by a worker
  // (which means it's now PROCESSING)
  if (it->second.status == JobStatus::QUEUED) {
    for (size_t i = 0; i < MAX_JOBS; ++i) {
      if (job_queue[i].ready && job_queue[i].claimed &&
          std::string(job_queue[i].job_id) == job_id) {
        it->second.status = JobStatus::PROCESSING;
        break;
      }
    }
  }

  switch (it->second.status) {
    case JobStatus::QUEUED: message = "Job is queued"; break;
    case JobStatus::PROCESSING: message = "Job is being processed"; break;
    case JobStatus::COMPLETED: message = "Job completed"; break;
    case JobStatus::FAILED: message = "Job failed: " + it->second.error_message; break;
    case JobStatus::CANCELLED: message = "Job was cancelled"; break;
    default: message = "Unknown status";
  }

  return it->second.status;
}

// Check if a job is MIP (vs LP)
bool get_job_is_mip(const std::string& job_id)
{
  std::lock_guard<std::mutex> lock(tracker_mutex);
  auto it = job_tracker.find(job_id);
  if (it == job_tracker.end()) {
    return false;  // Default to LP if not found
  }
  return it->second.is_mip;
}

// Get job result
bool get_job_result(const std::string& job_id,
                    std::vector<uint8_t>& result_data,
                    std::string& error_message)
{
  std::lock_guard<std::mutex> lock(tracker_mutex);
  auto it = job_tracker.find(job_id);

  if (it == job_tracker.end()) {
    error_message = "Job ID not found";
    return false;
  }

  if (it->second.status == JobStatus::COMPLETED) {
    result_data = it->second.result_data;
    return true;
  } else if (it->second.status == JobStatus::FAILED) {
    error_message = it->second.error_message;
    return false;
  } else {
    error_message = "Job not completed yet";
    return false;
  }
}

// Wait for job to complete (blocking)
// This uses condition variables - the thread will sleep until the job is done
bool wait_for_result(const std::string& job_id,
                     std::vector<uint8_t>& result_data,
                     std::string& error_message)
{
  // First check if job already completed
  {
    std::lock_guard<std::mutex> lock(tracker_mutex);
    auto it = job_tracker.find(job_id);

    if (it == job_tracker.end()) {
      error_message = "Job ID not found";
      return false;
    }

    // If already in terminal state, return immediately
    if (it->second.status == JobStatus::COMPLETED) {
      result_data = it->second.result_data;
      return true;
    } else if (it->second.status == JobStatus::FAILED) {
      error_message = it->second.error_message;
      return false;
    } else if (it->second.status == JobStatus::CANCELLED) {
      error_message = "Job was cancelled";
      return false;
    }
  }

  // Job is still running - reuse or create a shared waiter
  std::shared_ptr<JobWaiter> waiter;
  {
    std::lock_guard<std::mutex> lock(waiters_mutex);
    auto it = waiting_threads.find(job_id);
    if (it != waiting_threads.end()) {
      waiter = it->second;
    } else {
      waiter                  = std::make_shared<JobWaiter>();
      waiting_threads[job_id] = waiter;
    }
  }

  if (config.verbose) {
    std::cout << "[Server] WAIT_FOR_RESULT: waiting for job " << job_id << "\n";
  }

  waiter->waiters.fetch_add(1, std::memory_order_relaxed);

  // Wait on the condition variable - this thread will sleep until signaled
  {
    std::unique_lock<std::mutex> lock(waiter->mutex);
    waiter->cv.wait(lock, [&waiter] { return waiter->ready; });
  }

  if (config.verbose) {
    std::cout << "[Server] WAIT_FOR_RESULT: job " << job_id
              << " completed, success=" << waiter->success << "\n";
  }

  if (waiter->success) {
    if (waiter->waiters.load(std::memory_order_relaxed) > 1) {
      result_data = waiter->result_data;
    } else {
      result_data = std::move(waiter->result_data);
    }
    waiter->waiters.fetch_sub(1, std::memory_order_relaxed);
    return true;
  } else {
    error_message = waiter->error_message;
    waiter->waiters.fetch_sub(1, std::memory_order_relaxed);
    return false;
  }
}
void ensure_log_dir_exists()
{
  struct stat st;
  if (stat(LOG_DIR.c_str(), &st) != 0) { mkdir(LOG_DIR.c_str(), 0755); }
}

void ensure_upload_dir_exists()
{
  struct stat st;
  if (stat(UPLOAD_DIR.c_str(), &st) != 0) { mkdir(UPLOAD_DIR.c_str(), 0755); }
}

// Delete log file for a job
void delete_log_file(const std::string& job_id)
{
  std::string log_file = get_log_file_path(job_id);
  unlink(log_file.c_str());  // Ignore errors if file doesn't exist
}

void delete_upload_file(const std::string& upload_id)
{
  // Uploads may be stored as either ".bin" (protobuf payload) or ".bin.mps" (raw MPS payload).
  std::string f0 = get_upload_file_path(upload_id);
  unlink(f0.c_str());
  std::string f1 = f0 + ".mps";
  unlink(f1.c_str());
}

// Cancel job - returns: 0=success, 1=job_not_found, 2=already_completed, 3=already_cancelled
// Also returns the job's status after cancel attempt via job_status_out
int cancel_job(const std::string& job_id, JobStatus& job_status_out, std::string& message)
{
  std::lock_guard<std::mutex> lock(tracker_mutex);
  auto it = job_tracker.find(job_id);

  if (it == job_tracker.end()) {
    message        = "Job ID not found";
    job_status_out = JobStatus::NOT_FOUND;
    return 1;
  }

  JobStatus current_status = it->second.status;

  // Can't cancel completed jobs
  if (current_status == JobStatus::COMPLETED) {
    message        = "Cannot cancel completed job";
    job_status_out = JobStatus::COMPLETED;
    return 2;
  }

  // Already cancelled
  if (current_status == JobStatus::CANCELLED) {
    message        = "Job already cancelled";
    job_status_out = JobStatus::CANCELLED;
    return 3;
  }

  // Can't cancel failed jobs
  if (current_status == JobStatus::FAILED) {
    message        = "Cannot cancel failed job";
    job_status_out = JobStatus::FAILED;
    return 2;
  }

  // Find the job in the shared memory queue
  for (size_t i = 0; i < MAX_JOBS; ++i) {
    if (job_queue[i].ready && strcmp(job_queue[i].job_id, job_id.c_str()) == 0) {
      // Check if job is being processed by a worker
      pid_t worker_pid = job_queue[i].worker_pid;

      if (worker_pid > 0 && job_queue[i].claimed) {
        // Job is being processed - kill the worker
        if (config.verbose) {
          std::cout << "[Server] Cancelling running job " << job_id << " (killing worker "
                    << worker_pid << ")\n";
        }
        // Mark cancelled BEFORE killing so the worker monitor path (mark_worker_jobs_failed)
        // reliably observes was_cancelled=true and reports CANCELLED rather than FAILED.
        job_queue[i].cancelled = true;
        kill(worker_pid, SIGKILL);
      } else {
        // Job is queued but not yet claimed - mark as cancelled
        if (config.verbose) { std::cout << "[Server] Cancelling queued job " << job_id << "\n"; }
        job_queue[i].cancelled = true;
      }

      // Update job tracker
      it->second.status        = JobStatus::CANCELLED;
      it->second.error_message = "Job cancelled by user";
      job_status_out           = JobStatus::CANCELLED;
      message                  = "Job cancelled successfully";

      // Delete the log file for this job
      delete_log_file(job_id);

      // Wake up any threads waiting for this job
      {
        std::lock_guard<std::mutex> wlock(waiters_mutex);
        auto wit = waiting_threads.find(job_id);
        if (wit != waiting_threads.end()) {
          auto waiter = wit->second;
          {
            std::lock_guard<std::mutex> waiter_lock(waiter->mutex);
            waiter->error_message = "Job cancelled by user";
            waiter->success       = false;
            waiter->ready         = true;
          }
          waiter->cv.notify_all();
          waiting_threads.erase(wit);
        }
      }

      return 0;
    }
  }

  // Job not found in queue (might have already finished processing)
  // Re-check status since we hold the lock
  if (it->second.status == JobStatus::COMPLETED) {
    message        = "Cannot cancel completed job";
    job_status_out = JobStatus::COMPLETED;
    return 2;
  }

  // Job must be in flight or in an edge case - mark as cancelled anyway
  it->second.status        = JobStatus::CANCELLED;
  it->second.error_message = "Job cancelled by user";
  job_status_out           = JobStatus::CANCELLED;
  message                  = "Job cancelled";

  // Wake up any threads waiting for this job
  {
    std::lock_guard<std::mutex> wlock(waiters_mutex);
    auto wit = waiting_threads.find(job_id);
    if (wit != waiting_threads.end()) {
      auto waiter = wit->second;
      {
        std::lock_guard<std::mutex> waiter_lock(waiter->mutex);
        waiter->error_message = "Job cancelled by user";
        waiter->success       = false;
        waiter->ready         = true;
      }
      waiter->cv.notify_all();
      waiting_threads.erase(wit);
    }
  }

  return 0;
}

// ============================================================================
// Helper Functions
// ============================================================================

std::string generate_job_id()
{
  static std::random_device rd;
  static std::mt19937_64 gen(rd());
  static std::uniform_int_distribution<uint64_t> dis;

  std::stringstream ss;
  ss << std::hex << dis(gen);
  return ss.str();
}

// ============================================================================
// gRPC Service Implementation
// ============================================================================

class CuOptRemoteServiceImpl final : public cuopt::remote::CuOptRemoteService::Service {
 public:
  // SubmitJob - Submit LP/MIP job for async processing
  Status SubmitJob(ServerContext* context,
                   const cuopt::remote::SubmitJobRequest* request,
                   cuopt::remote::SubmitJobResponse* response) override
  {
    std::string job_id = generate_job_id();

    // Determine problem type and serialize request
    bool is_lp = request->has_lp_request();
    std::vector<uint8_t> job_data;

    if (is_lp) {
      // Serialize LP request
      auto& lp_req = request->lp_request();
      if (config.verbose) {
        std::cerr << "[gRPC] SubmitJob LP fields: bytes=" << lp_req.ByteSizeLong()
                  << " objective_scaling_factor=" << lp_req.problem().objective_scaling_factor()
                  << " objective_offset=" << lp_req.problem().objective_offset()
                  << " iteration_limit=" << lp_req.settings().iteration_limit()
                  << " method=" << lp_req.settings().method() << std::endl;
      }
      size_t size = lp_req.ByteSizeLong();
      job_data.resize(size);
      if (!lp_req.SerializeToArray(job_data.data(), size)) {
        return Status(StatusCode::INTERNAL, "Failed to serialize LP request");
      }
    } else if (request->has_mip_request()) {
      // Serialize MIP request
      auto& mip_req = request->mip_request();
      size_t size   = mip_req.ByteSizeLong();
      job_data.resize(size);
      if (!mip_req.SerializeToArray(job_data.data(), size)) {
        return Status(StatusCode::INTERNAL, "Failed to serialize MIP request");
      }
    } else {
      return Status(StatusCode::INVALID_ARGUMENT, "No problem data provided");
    }

    // Find and reserve a free slot in the shared job queue.
    //
    // NOTE: Unlike the legacy socket server (single-threaded accept loop),
    // gRPC can dispatch multiple SubmitJob RPCs concurrently. We must reserve
    // a slot so two SubmitJob calls don't pick the same entry.
    //
    // We use `claimed=true` as a temporary reservation while `ready=false`.
    int job_idx = -1;
    for (size_t i = 0; i < MAX_JOBS; ++i) {
      if (job_queue[i].ready.load()) { continue; }
      bool expected_claimed = false;
      if (job_queue[i].claimed.compare_exchange_strong(expected_claimed, true)) {
        job_idx = static_cast<int>(i);
        break;
      }
    }

    if (job_idx < 0) { return Status(StatusCode::RESOURCE_EXHAUSTED, "Job queue full"); }

    // Initialize job queue entry
    copy_cstr(job_queue[job_idx].job_id, job_id);
    job_queue[job_idx].problem_type = is_lp ? 0 : 1;
    job_queue[job_idx].data_size    = job_data.size();
    // `claimed` currently true as a reservation; keep it until the entry is fully initialized.
    job_queue[job_idx].cancelled.store(false);
    job_queue[job_idx].worker_index.store(-1);
    job_queue[job_idx].data_sent.store(false);
    job_queue[job_idx].shm_data_name[0] = '\0';  // Not used in pipe mode

    // Store job data for pipe mode
    {
      std::lock_guard<std::mutex> lock(pending_data_mutex);
      pending_job_data[job_id] = std::move(job_data);
      std::cout << "[DEBUG SubmitJob] Stored " << pending_job_data[job_id].size()
                << " bytes for job " << job_id << " in pending_job_data" << std::endl;
      std::cout.flush();
    }

    // Add to job tracker
    {
      std::lock_guard<std::mutex> lock(tracker_mutex);
      job_tracker[job_id] = JobInfo{
        job_id, JobStatus::QUEUED, std::chrono::steady_clock::now(), {}, {}, !is_lp, "", false};
    }

    // Publish job to workers: release reservation and set ready last.
    job_queue[job_idx].claimed.store(false);
    job_queue[job_idx].ready.store(true);

    response->set_job_id(job_id);
    response->set_message("Job submitted successfully");

    if (config.verbose) {
      std::cout << "[gRPC] Job submitted: " << job_id << " (type=" << (is_lp ? "LP" : "MIP") << ")"
                << std::endl;
    }

    return Status::OK;
  }

  Status UploadAndSubmit(ServerContext* context,
                         ServerReaderWriter<cuopt::remote::UploadJobResponse,
                                            cuopt::remote::UploadJobRequest>* stream) override
  {
    (void)context;

    if (!config.use_pipes) {
      return Status(StatusCode::FAILED_PRECONDITION,
                    "UploadAndSubmit currently requires pipe mode (do not use --use-shm)");
    }

    ensure_upload_dir_exists();

    cuopt::remote::UploadJobRequest in;
    cuopt::remote::UploadJobResponse out;

    const int64_t max_message_bytes =
      (config.max_message_mb <= 0) ? -1 : (static_cast<int64_t>(config.max_message_mb) * kMiB);
    auto set_upload_error =
      [&](const std::string& upload_id, const std::string& message, int64_t committed_size) {
        std::string full_message = message;
        if (full_message.find("max_message_mb=") == std::string::npos) {
          full_message += " (max_message_mb=" + std::to_string(config.max_message_mb) + ")";
        }
        out.Clear();
        auto* err = out.mutable_error();
        err->set_upload_id(upload_id);
        err->set_message(full_message);
        err->set_committed_size(committed_size);
        err->set_max_message_bytes(max_message_bytes);
      };

    // First message must be UploadStart.
    if (!stream->Read(&in) || !in.has_start()) {
      set_upload_error("", "First message must be UploadStart", 0);
      stream->Write(out);
      return Status(StatusCode::INVALID_ARGUMENT, "Missing UploadStart");
    }

    const auto& start       = in.start();
    std::string upload_id   = start.upload_id().empty() ? generate_job_id() : start.upload_id();
    bool is_mip             = (start.problem_type() == cuopt::remote::MIP);
    std::string upload_path = get_upload_file_path(upload_id);
    int64_t committed       = 0;

    const int64_t threshold_bytes = get_upload_mem_threshold_bytes();
    const int64_t total_size_hint = start.total_size();
    const bool force_file         = (threshold_bytes == 0) || start.resume();
    bool use_memory               = !force_file;
    if (threshold_bytes >= 0 && total_size_hint > 0 && total_size_hint > threshold_bytes) {
      use_memory = false;
    }
    if (threshold_bytes < 0) { use_memory = true; }

    int fd = -1;
    std::vector<uint8_t> mem_buffer;
    auto cleanup_file = [&]() {
      if (fd >= 0) {
        close(fd);
        delete_upload_file(upload_id);
        fd = -1;
      }
    };

    if (config.verbose) {
      std::cout << "[gRPC] UploadAndSubmit start upload_id=" << upload_id
                << " total_size=" << total_size_hint << " threshold_bytes=" << threshold_bytes
                << " resume=" << (start.resume() ? 1 : 0) << " use_memory=" << (use_memory ? 1 : 0)
                << " upload_path=" << upload_path << "\n";
      std::cout.flush();
    }

    auto open_upload_file = [&](bool resume) -> bool {
      int flags = O_CREAT | O_WRONLY;
      flags |= resume ? O_APPEND : O_TRUNC;
      fd = open(upload_path.c_str(), flags | O_CLOEXEC, 0600);
      if (fd < 0) {
        set_upload_error(
          upload_id, std::string("Failed to open upload file: ") + strerror(errno), committed);
        stream->Write(out);
        return false;
      }
      if (config.verbose) {
        struct stat st;
        if (fstat(fd, &st) == 0) {
          std::cout << "[gRPC] Upload file opened path=" << upload_path << " mode=" << std::oct
                    << (st.st_mode & 0777) << std::dec << " uid=" << st.st_uid
                    << " gid=" << st.st_gid << "\n";
        } else {
          std::cout << "[gRPC] Upload file opened path=" << upload_path
                    << " fstat_failed err=" << strerror(errno) << "\n";
        }
        std::cout.flush();
      }
      if (resume) {
        struct stat st;
        if (stat(upload_path.c_str(), &st) == 0) { committed = static_cast<int64_t>(st.st_size); }
      }
      return true;
    };

    if (!use_memory) {
      if (!open_upload_file(start.resume())) {
        return Status(StatusCode::INTERNAL, "Failed to open upload file");
      }
    }

    // Ack start with committed size (resume point).
    out.Clear();
    auto* ack = out.mutable_ack();
    ack->set_upload_id(upload_id);
    ack->set_committed_size(committed);
    ack->set_max_message_bytes(max_message_bytes);
    stream->Write(out);

    // Read chunks until finish.
    while (stream->Read(&in)) {
      if (in.has_chunk()) {
        const auto& ch = in.chunk();
        if (ch.upload_id() != upload_id) {
          set_upload_error(upload_id, "upload_id mismatch", committed);
          stream->Write(out);
          cleanup_file();
          return Status(StatusCode::INVALID_ARGUMENT, "upload_id mismatch");
        }
        if (ch.offset() != committed) {
          set_upload_error(upload_id, "Non-sequential chunk offset", committed);
          stream->Write(out);
          close(fd);
          return Status(StatusCode::OUT_OF_RANGE, "Non-sequential chunk offset");
        }

        const std::string& data = ch.data();
        if (!data.empty()) {
          if (use_memory) {
            // Switch to file if threshold exceeded or unknown size grows too large.
            if (threshold_bytes >= 0 &&
                committed + static_cast<int64_t>(data.size()) > threshold_bytes) {
              if (config.verbose) {
                std::cout << "[gRPC] Upload spill to disk upload_id=" << upload_id
                          << " committed=" << committed << " chunk=" << data.size()
                          << " threshold_bytes=" << threshold_bytes << "\n";
                std::cout.flush();
              }
              if (!open_upload_file(false)) {
                return Status(StatusCode::INTERNAL, "Failed to open upload file");
              }
              if (!mem_buffer.empty()) {
                if (!write_to_pipe(fd, mem_buffer.data(), mem_buffer.size())) {
                  set_upload_error(upload_id, "Failed to spill memory buffer to disk", committed);
                  stream->Write(out);
                  cleanup_file();
                  return Status(StatusCode::INTERNAL, "Failed to spill buffer");
                }
                mem_buffer.clear();
              }
              use_memory = false;
            }
          }

          if (use_memory) {
            mem_buffer.insert(mem_buffer.end(), data.begin(), data.end());
          } else {
            if (!write_to_pipe(fd, data.data(), data.size())) {
              set_upload_error(upload_id, "Failed to write chunk to disk", committed);
              stream->Write(out);
              cleanup_file();
              return Status(StatusCode::INTERNAL, "Failed to write chunk");
            }
          }
          committed += static_cast<int64_t>(data.size());
        }

        // Light progress logging for large uploads
        if (config.verbose && (committed % (256LL * kMiB) < static_cast<int64_t>(data.size()))) {
          std::cout << "[gRPC] Upload progress upload_id=" << upload_id
                    << " committed=" << committed << " bytes\n";
          std::cout.flush();
        }

        out.Clear();
        auto* chunk_ack = out.mutable_ack();
        chunk_ack->set_upload_id(upload_id);
        chunk_ack->set_committed_size(committed);
        chunk_ack->set_max_message_bytes(max_message_bytes);
        stream->Write(out);
        continue;
      }

      if (in.has_finish()) {
        const auto& fin = in.finish();
        if (fin.upload_id() != upload_id) {
          set_upload_error(upload_id, "upload_id mismatch on finish", committed);
          stream->Write(out);
          cleanup_file();
          return Status(StatusCode::INVALID_ARGUMENT, "upload_id mismatch on finish");
        }
        break;
      }

      set_upload_error(upload_id, "Unexpected message type during upload", committed);
      stream->Write(out);
      cleanup_file();
      return Status(StatusCode::INVALID_ARGUMENT, "Unexpected message type");
    }

    if (fd >= 0) { close(fd); }

    if (total_size_hint > 0 && committed != total_size_hint) {
      set_upload_error(upload_id,
                       std::string("Upload incomplete: committed size mismatch (max_message_mb=") +
                         std::to_string(config.max_message_mb) + ")",
                       committed);
      stream->Write(out);
      cleanup_file();
      return Status(StatusCode::OUT_OF_RANGE, "Upload incomplete: committed size mismatch");
    }

    // Enqueue job using file-backed payload or in-memory buffer
    std::string job_id = generate_job_id();

    int job_idx = -1;
    for (size_t i = 0; i < MAX_JOBS; ++i) {
      if (job_queue[i].ready.load()) { continue; }
      bool expected_claimed = false;
      if (job_queue[i].claimed.compare_exchange_strong(expected_claimed, true)) {
        job_idx = static_cast<int>(i);
        break;
      }
    }
    if (job_idx < 0) {
      set_upload_error(upload_id, "Job queue full", committed);
      stream->Write(out);
      cleanup_file();
      return Status(StatusCode::RESOURCE_EXHAUSTED, "Job queue full");
    }

    copy_cstr(job_queue[job_idx].job_id, job_id);
    job_queue[job_idx].problem_type = is_mip ? 1 : 0;
    job_queue[job_idx].data_size    = static_cast<uint64_t>(committed);
    job_queue[job_idx].cancelled.store(false);
    job_queue[job_idx].worker_index.store(-1);
    job_queue[job_idx].data_sent.store(false);
    job_queue[job_idx].shm_data_name[0] = '\0';
    if (use_memory) {
      std::lock_guard<std::mutex> lock(pending_data_mutex);
      pending_job_data[job_id] = std::move(mem_buffer);
    } else {
      std::lock_guard<std::mutex> lock(pending_files_mutex);
      pending_job_files[job_id] = PendingJobFile{upload_path, static_cast<uint64_t>(committed)};
    }

    if (config.verbose) {
      std::cout << "[gRPC] UploadAndSubmit stored payload upload_id=" << upload_id
                << " job_id=" << job_id << " bytes=" << committed
                << " storage=" << (use_memory ? "memory" : "file") << "\n";
      std::cout.flush();
    }

    {
      std::lock_guard<std::mutex> lock(tracker_mutex);
      job_tracker[job_id] = JobInfo{
        job_id, JobStatus::QUEUED, std::chrono::steady_clock::now(), {}, {}, is_mip, "", false};
    }

    job_queue[job_idx].claimed.store(false);
    job_queue[job_idx].ready.store(true);

    out.Clear();
    out.mutable_submit()->set_job_id(job_id);
    out.mutable_submit()->set_message("Job submitted successfully");
    stream->Write(out);

    if (config.verbose) {
      std::cout << "[gRPC] UploadAndSubmit enqueued job: " << job_id
                << " (type=" << (is_mip ? "MIP" : "LP") << ", bytes=" << committed << ")\n";
      std::cout.flush();
    }

    return Status::OK;
  }

  // CheckStatus - Check job status
  Status CheckStatus(ServerContext* context,
                     const cuopt::remote::StatusRequest* request,
                     cuopt::remote::StatusResponse* response) override
  {
    (void)context;
    std::string job_id = request->job_id();

    // Use shared-memory job queue state to expose PROCESSING when a worker claims the job.
    // This enables reliable mid-solve cancellation tests.
    std::string message;
    JobStatus status = check_job_status(job_id, message);

    switch (status) {
      case JobStatus::QUEUED: response->set_job_status(cuopt::remote::QUEUED); break;
      case JobStatus::PROCESSING: response->set_job_status(cuopt::remote::PROCESSING); break;
      case JobStatus::COMPLETED: response->set_job_status(cuopt::remote::COMPLETED); break;
      case JobStatus::FAILED: response->set_job_status(cuopt::remote::FAILED); break;
      case JobStatus::CANCELLED: response->set_job_status(cuopt::remote::CANCELLED); break;
      default: response->set_job_status(cuopt::remote::NOT_FOUND); break;
    }
    response->set_message(message);

    const int64_t max_bytes =
      (config.max_message_mb <= 0) ? -1 : (static_cast<int64_t>(config.max_message_mb) * kMiB);
    response->set_max_message_bytes(max_bytes);

    int64_t result_size_bytes = 0;
    if (status == JobStatus::COMPLETED) {
      std::lock_guard<std::mutex> lock(tracker_mutex);
      auto it = job_tracker.find(job_id);
      if (it != job_tracker.end()) { result_size_bytes = it->second.result_data.size(); }
    }
    response->set_result_size_bytes(result_size_bytes);

    return Status::OK;
  }

  // GetResult - Retrieve completed job result
  Status GetResult(ServerContext* context,
                   const cuopt::remote::GetResultRequest* request,
                   cuopt::remote::ResultResponse* response) override
  {
    std::string job_id = request->job_id();

    std::lock_guard<std::mutex> lock(tracker_mutex);
    auto it = job_tracker.find(job_id);

    if (it == job_tracker.end()) { return Status(StatusCode::NOT_FOUND, "Job not found"); }

    if (it->second.status != JobStatus::COMPLETED && it->second.status != JobStatus::FAILED) {
      return Status(StatusCode::UNAVAILABLE, "Result not ready");
    }

    if (it->second.status == JobStatus::FAILED) {
      response->set_error_message(it->second.error_message);
      return Status::OK;
    }

    // Parse result data
    if (it->second.is_mip) {
      cuopt::remote::MIPSolution mip_solution;
      if (!mip_solution.ParseFromArray(it->second.result_data.data(),
                                       it->second.result_data.size())) {
        return Status(StatusCode::INTERNAL, "Failed to parse MIP result");
      }
      response->mutable_mip_solution()->CopyFrom(mip_solution);
    } else {
      cuopt::remote::LPSolution lp_solution;
      if (!lp_solution.ParseFromArray(it->second.result_data.data(),
                                      it->second.result_data.size())) {
        return Status(StatusCode::INTERNAL, "Failed to parse LP result");
      }
      response->mutable_lp_solution()->CopyFrom(lp_solution);
    }

    if (config.verbose) { std::cout << "[gRPC] Result retrieved for job: " << job_id << std::endl; }

    return Status::OK;
  }

  Status StreamResult(ServerContext* context,
                      const cuopt::remote::GetResultRequest* request,
                      ServerWriter<cuopt::remote::ResultChunk>* writer) override
  {
    (void)context;
    std::string job_id = request->job_id();

    std::vector<uint8_t> bytes;
    bool is_mip = false;
    {
      std::lock_guard<std::mutex> lock(tracker_mutex);
      auto it = job_tracker.find(job_id);
      if (it == job_tracker.end()) {
        cuopt::remote::ResultChunk chunk;
        chunk.set_job_id(job_id);
        chunk.set_offset(0);
        chunk.set_done(true);
        chunk.set_error_message("Job not found");
        writer->Write(chunk);
        return Status::OK;
      }

      if (it->second.status != JobStatus::COMPLETED) {
        cuopt::remote::ResultChunk chunk;
        chunk.set_job_id(job_id);
        chunk.set_offset(0);
        chunk.set_done(true);
        chunk.set_error_message("Result not ready");
        writer->Write(chunk);
        return Status::OK;
      }

      bytes =
        it->second.result_data;  // copy; acceptable for now (can optimize with shared_ptr later)
      is_mip = it->second.is_mip;
    }

    const size_t chunk_size = kMiB;  // 1 MiB
    size_t offset           = 0;
    while (offset < bytes.size()) {
      size_t n = bytes.size() - offset;
      if (n > chunk_size) { n = chunk_size; }

      cuopt::remote::ResultChunk chunk;
      chunk.set_job_id(job_id);
      chunk.set_offset(static_cast<int64_t>(offset));
      chunk.set_data(reinterpret_cast<const char*>(bytes.data() + offset), n);
      chunk.set_done(false);

      if (!writer->Write(chunk)) { break; }  // client cancelled
      offset += n;
    }

    cuopt::remote::ResultChunk done;
    done.set_job_id(job_id);
    done.set_offset(static_cast<int64_t>(bytes.size()));
    done.set_done(true);
    // encode type hint in error_message is ugly; leave empty (client can infer by trying parse or
    // via status/is_mip call if needed). For now client will track is_mip separately.
    done.set_error_message("");
    writer->Write(done);

    if (config.verbose) {
      std::cout << "[gRPC] StreamResult finished job_id=" << job_id << " bytes=" << bytes.size()
                << " is_mip=" << (is_mip ? 1 : 0) << "\n";
      std::cout.flush();
    }

    return Status::OK;
  }

  // Other RPCs - stubs for now
  Status DeleteResult(ServerContext* context,
                      const cuopt::remote::DeleteRequest* request,
                      cuopt::remote::DeleteResponse* response) override
  {
    std::string job_id = request->job_id();

    {
      std::lock_guard<std::mutex> lock(tracker_mutex);
      job_tracker.erase(job_id);
    }

    delete_log_file(job_id);

    response->set_message("Result deleted");

    if (config.verbose) { std::cout << "[gRPC] Result deleted for job: " << job_id << std::endl; }

    return Status::OK;
  }

  Status CancelJob(ServerContext* context,
                   const cuopt::remote::CancelRequest* request,
                   cuopt::remote::CancelResponse* response) override
  {
    (void)context;
    std::string job_id = request->job_id();

    JobStatus internal_status = JobStatus::NOT_FOUND;
    std::string message;
    int rc = cancel_job(job_id, internal_status, message);

    // Map internal status -> protobuf JobStatus
    cuopt::remote::JobStatus pb_status = cuopt::remote::NOT_FOUND;
    switch (internal_status) {
      case JobStatus::QUEUED: pb_status = cuopt::remote::QUEUED; break;
      case JobStatus::PROCESSING: pb_status = cuopt::remote::PROCESSING; break;
      case JobStatus::COMPLETED: pb_status = cuopt::remote::COMPLETED; break;
      case JobStatus::FAILED: pb_status = cuopt::remote::FAILED; break;
      case JobStatus::CANCELLED: pb_status = cuopt::remote::CANCELLED; break;
      case JobStatus::NOT_FOUND: pb_status = cuopt::remote::NOT_FOUND; break;
    }

    response->set_job_status(pb_status);
    response->set_message(message);

    // Map rc -> ResponseStatus for backward-compatible response payload.
    // (We still return gRPC Status::OK; clients should check fields.)
    if (rc == 0 || rc == 3) {
      response->set_status(cuopt::remote::SUCCESS);
    } else if (rc == 1) {
      response->set_status(cuopt::remote::ERROR_NOT_FOUND);
    } else {
      response->set_status(cuopt::remote::ERROR_INVALID_REQUEST);
    }

    if (config.verbose) {
      std::cout << "[gRPC] CancelJob job_id=" << job_id << " rc=" << rc
                << " status=" << static_cast<int>(pb_status) << " msg=" << message << "\n";
      std::cout.flush();
    }

    return Status::OK;
  }

  Status WaitForResult(ServerContext* context,
                       const cuopt::remote::WaitRequest* request,
                       cuopt::remote::ResultResponse* response) override
  {
    (void)context;
    const std::string job_id = request->job_id();

    std::vector<uint8_t> result_data;
    std::string error_message;
    bool ok = wait_for_result(job_id, result_data, error_message);

    if (!ok) {
      response->set_status(cuopt::remote::ERROR_SOLVE_FAILED);
      response->set_error_message(error_message);
      return Status::OK;
    }

    response->set_status(cuopt::remote::SUCCESS);
    response->set_error_message("");

    // Determine LP vs MIP from job tracker
    bool is_mip = get_job_is_mip(job_id);
    if (is_mip) {
      cuopt::remote::MIPSolution mip_solution;
      if (!mip_solution.ParseFromArray(result_data.data(), result_data.size())) {
        response->set_status(cuopt::remote::ERROR_INTERNAL);
        response->set_error_message("Failed to parse MIP result");
        return Status::OK;
      }
      response->mutable_mip_solution()->CopyFrom(mip_solution);
    } else {
      cuopt::remote::LPSolution lp_solution;
      if (!lp_solution.ParseFromArray(result_data.data(), result_data.size())) {
        response->set_status(cuopt::remote::ERROR_INTERNAL);
        response->set_error_message("Failed to parse LP result");
        return Status::OK;
      }
      response->mutable_lp_solution()->CopyFrom(lp_solution);
    }

    if (config.verbose) {
      std::cout << "[gRPC] WaitForResult finished job_id=" << job_id
                << " bytes=" << result_data.size() << " is_mip=" << (is_mip ? 1 : 0) << "\n";
      std::cout.flush();
    }

    return Status::OK;
  }

  Status StreamLogs(ServerContext* context,
                    const cuopt::remote::StreamLogsRequest* request,
                    ServerWriter<cuopt::remote::LogMessage>* writer) override
  {
    const std::string job_id   = request->job_id();
    int64_t from_byte          = request->from_byte();
    const std::string log_path = get_log_file_path(job_id);

    // Wait for the log file to appear (job might not have started yet).
    int waited_ms = 0;
    while (!context->IsCancelled()) {
      struct stat st;
      if (stat(log_path.c_str(), &st) == 0) { break; }
      std::this_thread::sleep_for(std::chrono::milliseconds(50));
      waited_ms += 50;
      // Give up quickly if job doesn't exist.
      if (waited_ms >= 2000) {
        std::string msg;
        JobStatus s = check_job_status(job_id, msg);
        if (s == JobStatus::NOT_FOUND) {
          cuopt::remote::LogMessage m;
          m.set_line("Job not found");
          m.set_byte_offset(from_byte);
          m.set_job_complete(true);
          writer->Write(m);
          return Status::OK;
        }
        // else job exists but log not yet created; keep waiting
        waited_ms = 0;
      }
    }

    std::ifstream in(log_path, std::ios::in | std::ios::binary);
    if (!in.is_open()) {
      cuopt::remote::LogMessage m;
      m.set_line("Failed to open log file");
      m.set_byte_offset(from_byte);
      m.set_job_complete(true);
      writer->Write(m);
      return Status::OK;
    }

    if (from_byte > 0) { in.seekg(from_byte, std::ios::beg); }

    int64_t current_offset = from_byte;
    std::string line;

    while (!context->IsCancelled()) {
      std::streampos before = in.tellg();
      if (before >= 0) { current_offset = static_cast<int64_t>(before); }

      if (std::getline(in, line)) {
        // Account for the newline consumed by getline (1 byte) if present in file.
        std::streampos after = in.tellg();
        int64_t next_offset  = current_offset;
        if (after >= 0) {
          next_offset = static_cast<int64_t>(after);
        } else {
          // tellg can be -1 at EOF; approximate
          next_offset = current_offset + static_cast<int64_t>(line.size());
        }

        cuopt::remote::LogMessage m;
        m.set_line(line);
        m.set_byte_offset(next_offset);
        m.set_job_complete(false);
        if (!writer->Write(m)) { break; }
        continue;
      }

      // No new line available: clear EOF and sleep briefly
      if (in.eof()) {
        in.clear();
      } else if (in.fail()) {
        in.clear();
      }

      // If job is in terminal state and we've drained file, finish the stream.
      std::string msg;
      JobStatus s = check_job_status(job_id, msg);
      if (s == JobStatus::COMPLETED || s == JobStatus::FAILED || s == JobStatus::CANCELLED) {
        // One last attempt to read any remaining partial line
        std::streampos before2 = in.tellg();
        if (before2 >= 0) { current_offset = static_cast<int64_t>(before2); }
        if (std::getline(in, line)) {
          std::streampos after2 = in.tellg();
          int64_t next_offset2  = current_offset + static_cast<int64_t>(line.size());
          if (after2 >= 0) { next_offset2 = static_cast<int64_t>(after2); }
          cuopt::remote::LogMessage m;
          m.set_line(line);
          m.set_byte_offset(next_offset2);
          m.set_job_complete(false);
          writer->Write(m);
        }

        cuopt::remote::LogMessage done;
        done.set_line("");
        done.set_byte_offset(current_offset);
        done.set_job_complete(true);
        writer->Write(done);
        return Status::OK;
      }

      std::this_thread::sleep_for(std::chrono::milliseconds(100));
    }

    return Status::OK;
  }

  Status GetIncumbents(ServerContext* context,
                       const cuopt::remote::IncumbentRequest* request,
                       cuopt::remote::IncumbentResponse* response) override
  {
    (void)context;
    const std::string job_id = request->job_id();
    int64_t from_index       = request->from_index();
    int32_t max_count        = request->max_count();

    if (from_index < 0) { from_index = 0; }

    std::lock_guard<std::mutex> lock(tracker_mutex);
    auto it = job_tracker.find(job_id);
    if (it == job_tracker.end()) { return Status(StatusCode::NOT_FOUND, "Job not found"); }

    const auto& incumbents = it->second.incumbents;
    int64_t available      = static_cast<int64_t>(incumbents.size());
    if (from_index > available) { from_index = available; }

    int64_t count = available - from_index;
    if (max_count > 0 && count > max_count) { count = max_count; }

    for (int64_t i = 0; i < count; ++i) {
      const auto& inc = incumbents[static_cast<size_t>(from_index + i)];
      auto* out       = response->add_incumbents();
      out->set_index(from_index + i);
      out->set_objective(inc.objective);
      for (double v : inc.assignment) {
        out->add_assignment(v);
      }
      out->set_job_id(job_id);
    }

    response->set_next_index(available);
    bool done =
      (it->second.status == JobStatus::COMPLETED || it->second.status == JobStatus::FAILED ||
       it->second.status == JobStatus::CANCELLED);
    response->set_job_complete(done);
    if (config.verbose) {
      std::cout << "[gRPC] GetIncumbents job_id=" << job_id << " from=" << from_index
                << " returned=" << response->incumbents_size() << " next=" << available
                << " done=" << (done ? 1 : 0) << "\n";
      std::cout.flush();
    }
    return Status::OK;
  }
};

// ============================================================================
// Main
// ============================================================================

void print_usage(const char* prog)
{
  std::cout
    << "Usage: " << prog << " [options]\n"
    << "Options:\n"
    << "  -p, --port PORT         Listen port (default: 8765)\n"
    << "  -w, --workers NUM       Number of worker processes (default: 1)\n"
    << "      --use-shm           Use per-job shared memory for payload transfer (default: pipes)\n"
    << "      --max-message-mb N  gRPC max send/recv message size in MiB (default: 256, "
       "0=unlimited)\n"
    << "      --tls               Enable TLS (requires --tls-cert and --tls-key)\n"
    << "      --tls-cert PATH     Path to PEM-encoded server certificate\n"
    << "      --tls-key PATH      Path to PEM-encoded server private key\n"
    << "      --tls-root PATH     Path to PEM root certs for client verification\n"
    << "      --require-client-cert  Require and verify client certs (mTLS)\n"
    << "      --log-to-console    Enable solver log output to console (default: off)\n"
    << "  -q, --quiet             Reduce verbosity\n"
    << "  -h, --help              Show this help\n";
}

int main(int argc, char** argv)
{
  std::cerr << "[DEBUG] Starting cuopt_grpc_server main()" << std::endl;
  std::cerr.flush();

  // Parse arguments
  for (int i = 1; i < argc; i++) {
    std::string arg = argv[i];
    if (arg == "-p" || arg == "--port") {
      if (i + 1 < argc) { config.port = std::stoi(argv[++i]); }
    } else if (arg == "-w" || arg == "--workers") {
      if (i + 1 < argc) { config.num_workers = std::stoi(argv[++i]); }
    } else if (arg == "--use-shm") {
      config.use_pipes = false;
    } else if (arg == "--max-message-mb") {
      if (i + 1 < argc) { config.max_message_mb = std::stoi(argv[++i]); }
    } else if (arg == "--tls") {
      config.enable_tls = true;
    } else if (arg == "--tls-cert") {
      if (i + 1 < argc) { config.tls_cert_path = argv[++i]; }
    } else if (arg == "--tls-key") {
      if (i + 1 < argc) { config.tls_key_path = argv[++i]; }
    } else if (arg == "--tls-root") {
      if (i + 1 < argc) { config.tls_root_path = argv[++i]; }
    } else if (arg == "--require-client-cert") {
      config.require_client = true;
    } else if (arg == "--log-to-console") {
      config.log_to_console = true;
    } else if (arg == "-q" || arg == "--quiet") {
      config.verbose = false;
    } else if (arg == "-h" || arg == "--help") {
      print_usage(argv[0]);
      return 0;
    }
  }

  std::cerr << "[DEBUG] After argument parsing" << std::endl;
  std::cerr.flush();

  std::cout << "cuOpt gRPC Remote Solve Server\n"
            << "==============================\n"
            << "Port: " << config.port << "\n"
            << "Workers: " << config.num_workers << "\n"
            << std::endl;
  std::cout.flush();

  std::cerr << "[DEBUG] After printing header" << std::endl;
  std::cerr.flush();

  // Setup signal handling
  signal(SIGINT, signal_handler);
  signal(SIGTERM, signal_handler);

  // Create log directory
  ensure_log_dir_exists();
  ensure_upload_dir_exists();

  std::cerr << "[DEBUG] About to initialize shared memory" << std::endl;
  std::cerr.flush();

  // Avoid stale shared-memory state from prior crashed/force-killed runs.
  // Old worker processes may still have the old segments mapped; unlinking
  // here ensures this server creates fresh segments with clean state.
  shm_unlink(SHM_JOB_QUEUE);
  shm_unlink(SHM_RESULT_QUEUE);
  shm_unlink(SHM_CONTROL);

  // Initialize shared memory
  int shm_fd = shm_open(SHM_JOB_QUEUE, O_CREAT | O_RDWR, 0600);
  if (shm_fd < 0) {
    std::cerr << "[Server] Failed to create shared memory for job queue: " << strerror(errno)
              << "\n";
    return 1;
  }
  std::cerr << "[DEBUG] shm_open succeeded for job queue" << std::endl;
  std::cerr.flush();

  if (ftruncate(shm_fd, sizeof(JobQueueEntry) * MAX_JOBS) < 0) {
    std::cerr << "[Server] Failed to ftruncate job queue: " << strerror(errno) << "\n";
    close(shm_fd);
    return 1;
  }
  job_queue = static_cast<JobQueueEntry*>(
    mmap(nullptr, sizeof(JobQueueEntry) * MAX_JOBS, PROT_READ | PROT_WRITE, MAP_SHARED, shm_fd, 0));
  close(shm_fd);

  if (job_queue == MAP_FAILED) {
    std::cerr << "[Server] Failed to mmap job queue: " << strerror(errno) << "\n";
    return 1;
  }
  std::cerr << "[DEBUG] job_queue mmap succeeded" << std::endl;
  std::cerr.flush();

  int result_shm_fd = shm_open(SHM_RESULT_QUEUE, O_CREAT | O_RDWR, 0600);
  if (result_shm_fd < 0) {
    std::cerr << "[Server] Failed to create result queue shm: " << strerror(errno) << "\n";
    return 1;
  }
  if (ftruncate(result_shm_fd, sizeof(ResultQueueEntry) * MAX_RESULTS) < 0) {
    std::cerr << "[Server] Failed to ftruncate result queue: " << strerror(errno) << "\n";
    close(result_shm_fd);
    return 1;
  }
  result_queue = static_cast<ResultQueueEntry*>(mmap(nullptr,
                                                     sizeof(ResultQueueEntry) * MAX_RESULTS,
                                                     PROT_READ | PROT_WRITE,
                                                     MAP_SHARED,
                                                     result_shm_fd,
                                                     0));
  close(result_shm_fd);
  if (result_queue == MAP_FAILED) {
    std::cerr << "[Server] Failed to mmap result queue: " << strerror(errno) << "\n";
    return 1;
  }
  std::cerr << "[DEBUG] result_queue mmap succeeded" << std::endl;
  std::cerr.flush();

  int ctrl_shm_fd = shm_open(SHM_CONTROL, O_CREAT | O_RDWR, 0600);
  if (ctrl_shm_fd < 0) {
    std::cerr << "[Server] Failed to create control shm: " << strerror(errno) << "\n";
    return 1;
  }
  if (ftruncate(ctrl_shm_fd, sizeof(SharedMemoryControl)) < 0) {
    std::cerr << "[Server] Failed to ftruncate control: " << strerror(errno) << "\n";
    close(ctrl_shm_fd);
    return 1;
  }
  shm_ctrl = static_cast<SharedMemoryControl*>(
    mmap(nullptr, sizeof(SharedMemoryControl), PROT_READ | PROT_WRITE, MAP_SHARED, ctrl_shm_fd, 0));
  close(ctrl_shm_fd);
  if (shm_ctrl == MAP_FAILED) {
    std::cerr << "[Server] Failed to mmap control: " << strerror(errno) << "\n";
    return 1;
  }
  std::cerr << "[DEBUG] shm_ctrl mmap succeeded" << std::endl;
  std::cerr.flush();

  // Initialize shared memory
  for (size_t i = 0; i < MAX_JOBS; ++i) {
    memset(&job_queue[i], 0, sizeof(JobQueueEntry));
    job_queue[i].ready.store(false);
    job_queue[i].claimed.store(false);
    job_queue[i].cancelled.store(false);
    job_queue[i].worker_index.store(-1);
  }

  for (size_t i = 0; i < MAX_RESULTS; ++i) {
    memset(&result_queue[i], 0, sizeof(ResultQueueEntry));
    result_queue[i].ready.store(false);
    result_queue[i].retrieved.store(false);
  }

  shm_ctrl->shutdown_requested.store(false);
  shm_ctrl->active_workers.store(0);

  std::cerr << "[DEBUG] About to spawn workers" << std::endl;
  std::cerr.flush();

  // Spawn worker processes
  spawn_workers();

  std::cerr << "[DEBUG] spawn_workers() returned, " << worker_pids.size() << " workers spawned"
            << std::endl;
  std::cerr.flush();

  // Start result retrieval thread
  std::thread result_thread(result_retrieval_thread);

  // Start incumbent retrieval thread
  std::thread incumbent_thread(incumbent_retrieval_thread);

  // Start worker monitor thread
  std::thread monitor_thread(worker_monitor_thread);

  // Start gRPC server
  std::string server_address = "0.0.0.0:" + std::to_string(config.port);
  CuOptRemoteServiceImpl service;

  ServerBuilder builder;
  std::shared_ptr<grpc::ServerCredentials> creds;
  if (config.enable_tls) {
    if (config.tls_cert_path.empty() || config.tls_key_path.empty()) {
      std::cerr << "[Server] TLS enabled but --tls-cert/--tls-key not provided\n";
      return 1;
    }
    grpc::SslServerCredentialsOptions ssl_opts;
    grpc::SslServerCredentialsOptions::PemKeyCertPair key_cert;
    key_cert.cert_chain  = read_file_to_string(config.tls_cert_path);
    key_cert.private_key = read_file_to_string(config.tls_key_path);
    if (key_cert.cert_chain.empty() || key_cert.private_key.empty()) {
      std::cerr << "[Server] Failed to read TLS cert/key files\n";
      return 1;
    }
    ssl_opts.pem_key_cert_pairs.push_back(key_cert);

    if (!config.tls_root_path.empty()) {
      ssl_opts.pem_root_certs = read_file_to_string(config.tls_root_path);
      if (ssl_opts.pem_root_certs.empty()) {
        std::cerr << "[Server] Failed to read TLS root cert file\n";
        return 1;
      }
    }

    if (config.require_client) {
      if (ssl_opts.pem_root_certs.empty()) {
        std::cerr << "[Server] --require-client-cert requires --tls-root\n";
        return 1;
      }
      ssl_opts.client_certificate_request =
        GRPC_SSL_REQUEST_AND_REQUIRE_CLIENT_CERTIFICATE_AND_VERIFY;
    } else if (!ssl_opts.pem_root_certs.empty()) {
      ssl_opts.client_certificate_request = GRPC_SSL_REQUEST_CLIENT_CERTIFICATE_AND_VERIFY;
    }

    creds = grpc::SslServerCredentials(ssl_opts);
  } else {
    creds = grpc::InsecureServerCredentials();
  }

  builder.AddListeningPort(server_address, creds);
  builder.RegisterService(&service);
  // Allow large LP/MIP payloads (e.g. large MPS problems).
  // Note: gRPC uses -1 to mean unlimited.
  const int64_t max_bytes =
    (config.max_message_mb <= 0) ? -1 : (static_cast<int64_t>(config.max_message_mb) * kMiB);
  const int channel_limit =
    (max_bytes <= 0)
      ? -1
      : static_cast<int>(std::min<int64_t>(max_bytes, std::numeric_limits<int>::max()));
  builder.SetMaxReceiveMessageSize(channel_limit);
  builder.SetMaxSendMessageSize(channel_limit);

  std::unique_ptr<Server> server(builder.BuildAndStart());
  std::cout << "[gRPC Server] Listening on " << server_address << std::endl;
  std::cout << "[gRPC Server] Workers: " << config.num_workers << std::endl;
  std::cout << "[gRPC Server] Max message MiB: " << config.max_message_mb << std::endl;
  std::cout << "[gRPC Server] Press Ctrl+C to shutdown" << std::endl;

  // Wait for shutdown signal. We can't rely on signal handler to break server->Wait(),
  // so use a helper thread to call Shutdown() when keep_running flips false.
  std::thread shutdown_thread([&server]() {
    while (keep_running.load()) {
      std::this_thread::sleep_for(std::chrono::milliseconds(100));
    }
    if (server) { server->Shutdown(); }
  });

  server->Wait();
  if (shutdown_thread.joinable()) shutdown_thread.join();

  // Cleanup
  std::cout << "\n[Server] Shutting down..." << std::endl;
  keep_running                 = false;
  shm_ctrl->shutdown_requested = true;
  result_cv.notify_all();

  if (result_thread.joinable()) result_thread.join();
  if (incumbent_thread.joinable()) incumbent_thread.join();
  if (monitor_thread.joinable()) monitor_thread.join();

  wait_for_workers();
  cleanup_shared_memory();

  std::cout << "[Server] Shutdown complete" << std::endl;
  return 0;
}

#else  // !CUOPT_ENABLE_GRPC

#include <iostream>

int main()
{
  std::cerr << "Error: cuopt_grpc_server requires gRPC support.\n"
            << "Rebuild with gRPC enabled (CUOPT_ENABLE_GRPC=ON)" << std::endl;
  return 1;
}

#endif  // CUOPT_ENABLE_GRPC
