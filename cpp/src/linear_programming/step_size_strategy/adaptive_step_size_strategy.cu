/* clang-format off */
/*
 * SPDX-FileCopyrightText: Copyright (c) 2022-2026, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 */
/* clang-format on */

#include <cuopt/linear_programming/pdlp/pdlp_hyper_params.cuh>

#include <linear_programming/pdlp_climber_strategy.hpp>
#include <linear_programming/pdlp_constants.hpp>
#include <linear_programming/step_size_strategy/adaptive_step_size_strategy.hpp>
#include <linear_programming/swap_and_resize_helper.cuh>
#include <linear_programming/utils.cuh>

#include <mip/mip_constants.hpp>

#include <utilities/unique_pinned_ptr.hpp>

#include <raft/sparse/detail/cusparse_macros.h>
#include <raft/sparse/detail/cusparse_wrappers.h>
#include <raft/common/nvtx.hpp>
#include <raft/core/operators.hpp>
#include <raft/linalg/binary_op.cuh>
#include <raft/linalg/detail/cublas_wrappers.hpp>
#include <raft/util/cuda_utils.cuh>
#include <raft/util/cudart_utils.hpp>

#include <cub/cub.cuh>

#include <limits>

namespace cuopt::linear_programming::detail {

constexpr int parallel_stream_computation = 2;

template <typename i_t, typename f_t>
adaptive_step_size_strategy_t<i_t, f_t>::adaptive_step_size_strategy_t(
  raft::handle_t const* handle_ptr,
  rmm::device_uvector<f_t>* primal_weight,
  rmm::device_uvector<f_t>* step_size,
  bool is_legacy_batch_mode,
  i_t primal_size,
  i_t dual_size,
  const std::vector<pdlp_climber_strategy_t>& climber_strategies,
  const pdlp_hyper_params::pdlp_hyper_params_t& hyper_params)
  : batch_mode_(climber_strategies.size() > 1),
    stream_pool_(parallel_stream_computation),
    dot_delta_X_(cudaEventDisableTiming),
    dot_delta_Y_(cudaEventDisableTiming),
    deltas_are_done_(cudaEventDisableTiming),
    handle_ptr_(handle_ptr),
    stream_view_(handle_ptr_->get_stream()),
    primal_size_(primal_size),
    dual_size_(dual_size),
    primal_weight_(primal_weight),
    step_size_(step_size),
    valid_step_size_(1),
    interaction_{climber_strategies.size(), stream_view_},
    norm_squared_delta_primal_{climber_strategies.size(), stream_view_},
    norm_squared_delta_dual_{climber_strategies.size(), stream_view_},
    reusable_device_scalar_value_1_{f_t(1.0), stream_view_},
    reusable_device_scalar_value_0_{f_t(0.0), stream_view_},
    dot_product_storage(0, stream_view_),
    graph(stream_view_, is_legacy_batch_mode),
    climber_strategies_(climber_strategies),
    hyper_params_(hyper_params)
{
  valid_step_size_[0] = 0;

  if (batch_mode_) {
    // Pass down any input pointer of the right type, actual pointer does not matter
    size_t byte_needed = 0;
    RAFT_CUDA_TRY(cub::DeviceSegmentedReduce::Sum(
      nullptr,
      byte_needed,
      thrust::make_transform_iterator(thrust::make_zip_iterator(norm_squared_delta_primal_.data(),
                                                                norm_squared_delta_primal_.data()),
                                      tuple_multiplies<f_t>{}),
      interaction_.data(),
      climber_strategies_.size(),
      primal_size_,
      stream_view_));
    dot_product_bytes = std::max(dot_product_bytes, byte_needed);

    RAFT_CUDA_TRY(cub::DeviceSegmentedReduce::Sum(
      nullptr,
      byte_needed,
      thrust::make_transform_iterator(norm_squared_delta_primal_.data(), power_two_func_t<f_t>{}),
      norm_squared_delta_primal_.data(),
      climber_strategies_.size(),
      primal_size_,
      stream_view_));
    dot_product_bytes = std::max(dot_product_bytes, byte_needed);

    RAFT_CUDA_TRY(cub::DeviceSegmentedReduce::Sum(
      nullptr,
      byte_needed,
      thrust::make_transform_iterator(norm_squared_delta_dual_.data(), power_two_func_t<f_t>{}),
      norm_squared_delta_dual_.data(),
      climber_strategies_.size(),
      dual_size_,
      stream_view_));
    dot_product_bytes = std::max(dot_product_bytes, byte_needed);

    dot_product_storage.resize(dot_product_bytes, stream_view_);
  }
}

template <typename i_t, typename f_t>
__global__ void adaptive_step_size_swap_device_vectors_kernel(
  const swap_pair_t<i_t>* swap_pairs,
  i_t swap_count,
  raft::device_span<f_t> interaction,
  raft::device_span<f_t> norm_squared_delta_primal,
  raft::device_span<f_t> norm_squared_delta_dual)
{
  const i_t idx = static_cast<i_t>(blockIdx.x * blockDim.x + threadIdx.x);
  if (idx >= swap_count) { return; }

  const i_t left  = swap_pairs[idx].left;
  const i_t right = swap_pairs[idx].right;

  cuda::std::swap(interaction[left], interaction[right]);
  cuda::std::swap(norm_squared_delta_primal[left], norm_squared_delta_primal[right]);
  cuda::std::swap(norm_squared_delta_dual[left], norm_squared_delta_dual[right]);
}

template <typename i_t, typename f_t>
void adaptive_step_size_strategy_t<i_t, f_t>::swap_context(
  const thrust::universal_host_pinned_vector<swap_pair_t<i_t>>& swap_pairs)
{
  if (swap_pairs.empty()) { return; }

  const auto batch_size = static_cast<i_t>(interaction_.size());
  cuopt_assert(batch_size > 0, "Batch size must be greater than 0");
  for (const auto& pair : swap_pairs) {
    cuopt_assert(pair.left < pair.right, "Left swap index must be less than right swap index");
    cuopt_assert(pair.left < batch_size, "Left swap index is out of bounds");
    cuopt_assert(pair.right < batch_size, "Right swap index is out of bounds");
  }

  const auto [grid_size, block_size] =
    kernel_config_from_batch_size(static_cast<i_t>(swap_pairs.size()));
  adaptive_step_size_swap_device_vectors_kernel<i_t, f_t>
    <<<grid_size, block_size, 0, stream_view_>>>(thrust::raw_pointer_cast(swap_pairs.data()),
                                                 static_cast<i_t>(swap_pairs.size()),
                                                 make_span(interaction_),
                                                 make_span(norm_squared_delta_primal_),
                                                 make_span(norm_squared_delta_dual_));
  RAFT_CUDA_TRY(cudaPeekAtLastError());
}

template <typename i_t, typename f_t>
void adaptive_step_size_strategy_t<i_t, f_t>::resize_context(i_t new_size)
{
  [[maybe_unused]] const auto batch_size = static_cast<i_t>(interaction_.size());
  cuopt_assert(batch_size > 0, "Batch size must be greater than 0");
  cuopt_assert(new_size > 0, "New size must be greater than 0");
  cuopt_assert(new_size < batch_size, "New size must be less than batch size");

  interaction_.resize(new_size, stream_view_);
  norm_squared_delta_primal_.resize(new_size, stream_view_);
  norm_squared_delta_dual_.resize(new_size, stream_view_);
}

template <typename i_t, typename f_t>
__global__ void compute_step_sizes_from_movement_and_interaction(
  typename adaptive_step_size_strategy_t<i_t, f_t>::view_t step_size_strategy_view,
  f_t* primal_step_size,
  f_t* dual_step_size,
  i_t* pdhg_iteration)
{
  if (threadIdx.x + blockIdx.x * blockDim.x > 0) { return; }

  cuopt_assert(step_size_strategy_view.primal_weight.size() == 1,
               "compute_step_sizes_from_movement_and_interaction not supported in batch");

  const f_t primal_weight = step_size_strategy_view.primal_weight[0];

  const f_t movement =
    step_size_strategy_view.hyper_params.primal_distance_smoothing * primal_weight *
      *step_size_strategy_view.norm_squared_delta_primal +
    (step_size_strategy_view.hyper_params.dual_distance_smoothing / primal_weight) *
      *step_size_strategy_view.norm_squared_delta_dual;

#ifdef PDLP_DEBUG_MODE
  printf("-compute_step_sizes_from_movement_and_interaction:\n");
#endif
  if (movement <= 0 || movement >= divergent_movement<f_t>) {
    *step_size_strategy_view.valid_step_size = -1;
#ifdef PDLP_DEBUG_MODE
    printf("  Movement is %lf. Done or numerical error has happened\n", movement);
#endif
    return;
  }

  const f_t interaction = raft::abs(*step_size_strategy_view.interaction);
  f_t step_size         = step_size_strategy_view.step_size[0];

  // Increase PDHG iteration
  *pdhg_iteration += 1;

  f_t iteration_coefficient_ = *pdhg_iteration;

  // proof of thm 1 requires movement / step_size >= interaction.
  f_t step_size_limit = interaction > 0.0 ? movement / interaction : raft::myInf<f_t>();

#ifdef PDLP_DEBUG_MODE
  printf("    interaction=%lf movement=%lf\n", interaction, movement);
  printf("    step_size_=%lf step_size_limit=%lf pdhg_iteration=%d iteration_coefficient_=%lf\n",
         step_size,
         step_size_limit,
         *pdhg_iteration,
         iteration_coefficient_);
#endif

  if (step_size <= step_size_limit) {
    *step_size_strategy_view.valid_step_size = 1;

#ifdef PDLP_DEBUG_MODE
    printf("    Step size is smaller\n");
#endif
  }

  // The step size was too large and therefore we now compute the next stepsize to test out.
  // We have two candidates of which we take the smaller to retry taking a step
  const f_t potential_new_step_size_1 =
    (f_t(1.0) - raft::pow<f_t>(iteration_coefficient_ + f_t(1.0),
                               -step_size_strategy_view.hyper_params.reduction_exponent)) *
    step_size_limit;
  const f_t potential_new_step_size_2 =
    (f_t(1.0) + raft::pow<f_t>(iteration_coefficient_ + f_t(1.0),
                               -step_size_strategy_view.hyper_params.growth_exponent)) *
    step_size;

#ifdef PDLP_DEBUG_MODE
  printf(
    "Compute adaptative step size: iteration_coefficient_=%lf "
    "-hyper_params.reduction_exponent=%lf step_size_limit=%lf\n",
    iteration_coefficient_,
    -step_size_strategy_view.hyper_params.reduction_exponent,
    step_size_limit);
  printf(
    "Compute adaptative step size: iteration_coefficient_=%lf "
    "-hyper_params.growth_exponent=%lf step_size_=%lf\n",
    iteration_coefficient_,
    -step_size_strategy_view.hyper_params.growth_exponent,
    step_size);
  printf(
    "Compute adaptative step size: potential_new_step_size_1=%lf potential_new_step_size_2=%lf\n",
    potential_new_step_size_1,
    potential_new_step_size_2);
#endif

  step_size = raft::min<f_t>(potential_new_step_size_1, potential_new_step_size_2);

#ifdef PDLP_DEBUG_MODE
  printf("Compute adaptative step size: min_step_size_picked=%lf\n", step_size);
#endif

  *primal_step_size = step_size / primal_weight;
  *dual_step_size   = step_size * primal_weight;

  step_size_strategy_view.step_size[0] = step_size;
  cuopt_assert(!isnan(step_size), "step size can't be nan");
  cuopt_assert(!isinf(step_size), "step size can't be inf");
}

template <typename i_t, typename f_t>
i_t adaptive_step_size_strategy_t<i_t, f_t>::get_valid_step_size() const
{
  return valid_step_size_[0];
}

template <typename i_t, typename f_t>
f_t adaptive_step_size_strategy_t<i_t, f_t>::get_interaction(i_t i) const
{
  return interaction_.element(i, stream_view_);
}

template <typename i_t, typename f_t>
f_t adaptive_step_size_strategy_t<i_t, f_t>::get_norm_squared_delta_primal(i_t i) const
{
  return norm_squared_delta_primal_.element(i, stream_view_);
}

template <typename i_t, typename f_t>
f_t adaptive_step_size_strategy_t<i_t, f_t>::get_norm_squared_delta_dual(i_t i) const
{
  return norm_squared_delta_dual_.element(i, stream_view_);
}

template <typename i_t, typename f_t>
const rmm::device_uvector<f_t>& adaptive_step_size_strategy_t<i_t, f_t>::get_interaction() const
{
  return interaction_;
}

template <typename i_t, typename f_t>
const rmm::device_uvector<f_t>&
adaptive_step_size_strategy_t<i_t, f_t>::get_norm_squared_delta_primal() const
{
  return norm_squared_delta_primal_;
}

template <typename i_t, typename f_t>
const rmm::device_uvector<f_t>&
adaptive_step_size_strategy_t<i_t, f_t>::get_norm_squared_delta_dual() const
{
  return norm_squared_delta_dual_;
}

template <typename i_t, typename f_t>
void adaptive_step_size_strategy_t<i_t, f_t>::set_valid_step_size(i_t valid)
{
  valid_step_size_[0] = valid;
}

template <typename i_t, typename f_t>
void adaptive_step_size_strategy_t<i_t, f_t>::compute_step_sizes(
  pdhg_solver_t<i_t, f_t>& pdhg_solver,
  rmm::device_uvector<f_t>& primal_step_size,
  rmm::device_uvector<f_t>& dual_step_size,
  i_t total_pdlp_iterations)
{
  raft::common::nvtx::range fun_scope("compute_step_sizes");

  cuopt_assert(!batch_mode_, "Batch mode is not supported for compute_step_sizes");

  if (!graph.is_initialized(total_pdlp_iterations)) {
    graph.start_capture(total_pdlp_iterations);

    // compute numerator and deminator of n_lim
    compute_interaction_and_movement(pdhg_solver.get_primal_tmp_resource(),
                                     pdhg_solver.get_cusparse_view(),
                                     pdhg_solver.get_saddle_point_state());
    // Compute n_lim, n_next and decide if step size is valid
    compute_step_sizes_from_movement_and_interaction<i_t, f_t>
      <<<1, 1, 0, stream_view_>>>(this->view(),
                                  primal_step_size.data(),
                                  dual_step_size.data(),
                                  pdhg_solver.get_d_total_pdhg_iterations().data());
    graph.end_capture(total_pdlp_iterations);
  }
  graph.launch(total_pdlp_iterations);
  // Steam sync so that next call can see modification made to host var valid_step_size
  RAFT_CUDA_TRY(cudaStreamSynchronize(stream_view_));
}

template <typename i_t, typename f_t>
void adaptive_step_size_strategy_t<i_t, f_t>::compute_interaction_and_movement(
  rmm::device_uvector<f_t>& tmp_primal,
  cusparse_view_t<i_t, f_t>& cusparse_view,
  saddle_point_state_t<i_t, f_t>& current_saddle_point_state)
{
  // QP would need this:
  // if iszero(problem.objective_matrix)
  //   primal_objective_interaction = 0.0
  // else
  //   primal_objective_interaction =
  //     0.5 * (delta_primal' * problem.objective_matrix * delta_primal)
  // end
  // would need to add abs(primal_objective_interaction) to interaction as well

  /*
    Here we compute : movement / interaction

    Movement: ||(x' - x), (y' - y)||Â²
    Interaction: (y' - y)_t . A @ (x' - x)

    Deltas x & y were computed during pdhg step

    We will compute in parallel (parallel cuda graph):
    ||(x' - x)||
    ||(y' - y)||
    (y' - y)_t . A @ (x' - x)

    And finally merge the results
  */

  // We need to make sure both dot products happens after previous operations (next_primal/dual)
  // Thus, we add another node in the main stream before starting the SpMVs

  if (!batch_mode_) deltas_are_done_.record(stream_view_);

  // primal_dual_interaction computation => we purposly diverge from the paper (delta_y . (A @ x' -
  // A@x)) to save one SpMV
  // Instead we do: delta_x . (A_t @ y' - A_t @ y)
  // A_t @ y has already been computed during compute next_primal
  // A_t @ y' is computed here each time but, if a valid step is found, A @ y'
  // becomes A @ y for next step (as what was y' becomes y if valid for next step). This saves the
  // first A @ y SpMV in the compute_next_primal of next PDHG step

  // Compute A_t @ (y' - y) = A_t @ y' - 1 * current_AtY

  // First compute Ay' to be reused as Ay in next PDHG iteration (if found step size if valid)
  if (!batch_mode_) {
    RAFT_CUSPARSE_TRY(
      raft::sparse::detail::cusparsespmv(handle_ptr_->get_cusparse_handle(),
                                         CUSPARSE_OPERATION_NON_TRANSPOSE,
                                         reusable_device_scalar_value_1_.data(),  // alpha
                                         cusparse_view.A_T,
                                         cusparse_view.potential_next_dual_solution,
                                         reusable_device_scalar_value_0_.data(),  // beta
                                         cusparse_view.next_AtY,
                                         CUSPARSE_SPMV_CSR_ALG2,
                                         (f_t*)cusparse_view.buffer_transpose.data(),
                                         stream_view_));
  } else {
    // TODO later batch mode: handle if not all restart
    RAFT_CUSPARSE_TRY(
      raft::sparse::detail::cusparsespmm(handle_ptr_->get_cusparse_handle(),
                                         CUSPARSE_OPERATION_NON_TRANSPOSE,
                                         CUSPARSE_OPERATION_NON_TRANSPOSE,
                                         reusable_device_scalar_value_1_.data(),
                                         cusparse_view.A_T,
                                         cusparse_view.batch_potential_next_dual_solution,
                                         reusable_device_scalar_value_0_.data(),
                                         cusparse_view.batch_next_AtYs,
                                         CUSPARSE_SPMM_CSR_ALG3,
                                         (f_t*)cusparse_view.buffer_transpose_batch.data(),
                                         stream_view_));
  }

  // Compute Ay' - Ay = next_Aty - current_Aty
  // TODO later batch mode: remove this once you want to do per climber restart
  cub::DeviceTransform::Transform(
    cuda::std::make_tuple(current_saddle_point_state.get_next_AtY().data(),
                          current_saddle_point_state.get_current_AtY().data()),
    tmp_primal.data(),
    tmp_primal.size(),
    cuda::std::minus<>{},
    stream_view_.value());

  if (!batch_mode_) {
    // compute interaction (x'-x) . (A(y'-y))
    RAFT_CUBLAS_TRY(
      raft::linalg::detail::cublasdot(handle_ptr_->get_cublas_handle(),
                                      current_saddle_point_state.get_primal_size(),
                                      tmp_primal.data(),
                                      primal_stride,
                                      current_saddle_point_state.get_delta_primal().data(),
                                      primal_stride,
                                      interaction_.data(),
                                      stream_view_));

    // Compute movement
    //  compute euclidean norm squared which is
    //  same as taking the dot product with itself
    //    movement = 0.5 * solver_state.primal_weight
    //    * norm(delta_primal) ^
    //               2 + (0.5 /
    //               solver_state.primal_weight) *
    //               norm(delta_dual) ^ 2;
    deltas_are_done_.stream_wait(stream_pool_.get_stream(0));
    RAFT_CUBLAS_TRY(
      raft::linalg::detail::cublasdot(handle_ptr_->get_cublas_handle(),
                                      current_saddle_point_state.get_primal_size(),
                                      current_saddle_point_state.get_delta_primal().data(),
                                      primal_stride,
                                      current_saddle_point_state.get_delta_primal().data(),
                                      primal_stride,
                                      norm_squared_delta_primal_.data(),
                                      stream_pool_.get_stream(0)));
    dot_delta_X_.record(stream_pool_.get_stream(0));

    deltas_are_done_.stream_wait(stream_pool_.get_stream(1));
    RAFT_CUBLAS_TRY(
      raft::linalg::detail::cublasdot(handle_ptr_->get_cublas_handle(),
                                      current_saddle_point_state.get_dual_size(),
                                      current_saddle_point_state.get_delta_dual().data(),
                                      dual_stride,
                                      current_saddle_point_state.get_delta_dual().data(),
                                      dual_stride,
                                      norm_squared_delta_dual_.data(),
                                      stream_pool_.get_stream(1)));
    dot_delta_Y_.record(stream_pool_.get_stream(1));

    // Wait on main stream for both dot to be done before launching the next kernel
    dot_delta_X_.stream_wait(stream_view_);
    dot_delta_Y_.stream_wait(stream_view_);
  } else {
    // TODO later batch mode: remove this once you want to do per climber restart
    cub::DeviceSegmentedReduce::Sum(
      dot_product_storage.data(),
      dot_product_bytes,
      thrust::make_transform_iterator(
        thrust::make_zip_iterator(tmp_primal.data(),
                                  current_saddle_point_state.get_delta_primal().data()),
        tuple_multiplies<f_t>{}),
      interaction_.data(),
      climber_strategies_.size(),
      primal_size_,
      stream_view_);

    cub::DeviceSegmentedReduce::Sum(
      dot_product_storage.data(),
      dot_product_bytes,
      thrust::make_transform_iterator(current_saddle_point_state.get_delta_primal().data(),
                                      power_two_func_t<f_t>{}),
      norm_squared_delta_primal_.data(),
      climber_strategies_.size(),
      primal_size_,
      stream_view_);

    cub::DeviceSegmentedReduce::Sum(
      dot_product_storage.data(),
      dot_product_bytes,
      thrust::make_transform_iterator(current_saddle_point_state.get_delta_dual().data(),
                                      power_two_func_t<f_t>{}),
      norm_squared_delta_dual_.data(),
      climber_strategies_.size(),
      dual_size_,
      stream_view_);
  }
}

template <typename i_t, typename f_t>
__global__ void compute_actual_stepsizes(
  const typename adaptive_step_size_strategy_t<i_t, f_t>::view_t step_size_strategy_view,
  raft::device_span<f_t> primal_step_size,
  raft::device_span<f_t> dual_step_size,
  i_t batch_size)
{
  const int idx = threadIdx.x + blockIdx.x * blockDim.x;
  if (idx >= batch_size) { return; }

  const f_t step_size     = step_size_strategy_view.step_size[idx];
  const f_t primal_weight = step_size_strategy_view.primal_weight[idx];

  cuopt_assert(!isnan(step_size), "step size can't be nan");
  cuopt_assert(!isinf(step_size), "step size can't be inf");
  cuopt_assert(!isnan(primal_weight), "primal weight can't be nan");
  cuopt_assert(!isinf(primal_weight), "primal weight can't be inf");
  cuopt_assert(primal_weight != f_t(0.0), "primal weight must be non-zero");

  primal_step_size[idx] = step_size / primal_weight;
  dual_step_size[idx]   = step_size * primal_weight;
}

template <typename i_t, typename f_t>
void adaptive_step_size_strategy_t<i_t, f_t>::get_primal_and_dual_stepsizes(
  rmm::device_uvector<f_t>& primal_step_size, rmm::device_uvector<f_t>& dual_step_size)
{
  const auto [grid_size, block_size] = kernel_config_from_batch_size(climber_strategies_.size());
  cuopt_assert(primal_step_size.size() == climber_strategies_.size(),
               "primal step size must be the same size as the number of climber strategies");
  cuopt_assert(dual_step_size.size() == climber_strategies_.size(),
               "dual step size must be the same size as the number of climber strategies");
  cuopt_assert(primal_weight_ != nullptr, "primal weight must be non-null");
  cuopt_assert(primal_weight_->size() == climber_strategies_.size(),
               "primal weight must be the same size as the number of climber strategies");
  cuopt_assert(step_size_ != nullptr, "step size must be non-null");
  cuopt_assert(step_size_->size() == climber_strategies_.size(),
               "step size must be the same size as the number of climber strategies");
  compute_actual_stepsizes<i_t, f_t>
    <<<grid_size, block_size, 0, stream_view_>>>(this->view(),
                                                 make_span(primal_step_size),
                                                 make_span(dual_step_size),
                                                 climber_strategies_.size());
  RAFT_CUDA_TRY(cudaPeekAtLastError());
}

template <typename i_t, typename f_t>
typename adaptive_step_size_strategy_t<i_t, f_t>::view_t
adaptive_step_size_strategy_t<i_t, f_t>::view()
{
  adaptive_step_size_strategy_t<i_t, f_t>::view_t v{};

  v.primal_weight   = make_span(*primal_weight_);
  v.step_size       = make_span(*step_size_);
  v.valid_step_size = thrust::raw_pointer_cast(valid_step_size_.data());

  v.interaction = interaction_.data();

  v.norm_squared_delta_primal = norm_squared_delta_primal_.data();
  v.norm_squared_delta_dual   = norm_squared_delta_dual_.data();

  v.hyper_params = hyper_params_;

  return v;
}

#define INSTANTIATE(F_TYPE)                                                                    \
  template class adaptive_step_size_strategy_t<int, F_TYPE>;                                   \
  template __global__ void compute_actual_stepsizes<int, F_TYPE>(                              \
    const typename adaptive_step_size_strategy_t<int, F_TYPE>::view_t step_size_strategy_view, \
    raft::device_span<F_TYPE> primal_step_size,                                                \
    raft::device_span<F_TYPE> dual_step_size,                                                  \
    int size);                                                                                 \
                                                                                               \
  template __global__ void compute_step_sizes_from_movement_and_interaction<int, F_TYPE>(      \
    typename adaptive_step_size_strategy_t<int, F_TYPE>::view_t step_size_strategy_view,       \
    F_TYPE * primal_step_size,                                                                 \
    F_TYPE * dual_step_size,                                                                   \
    int* pdhg_iteration);

#if MIP_INSTANTIATE_FLOAT
INSTANTIATE(float)
#endif

#if MIP_INSTANTIATE_DOUBLE
INSTANTIATE(double)
#endif

}  // namespace cuopt::linear_programming::detail
